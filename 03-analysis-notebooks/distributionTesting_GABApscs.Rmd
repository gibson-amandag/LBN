---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
pscProps <- loadExcelSheet(dataFolder, LBN_DataName, "pscProps")

# pscProps <- pscProps %>%
#   makeFactors(
#     cols = cellID
#   )

pscProps <- pscProps %>%
  left_join(
    GABApscs_240 %>%
      select(
        cellID
        , mouseID
        , damID
        , earlyLifeTrt
        , adultTrt
      )
    , by = "cellID"
  )

saveDFsToCSV(
  saveFolder = "./"
  ,"pscProps" = pscProps
)
```

```{r}
pscProps %>%
  group_by(
    cellID
  ) %>%
  meanSummary(
    relPeak
  )
```


```{r}
GABApscs_240
```


Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.


Quantile summary

```{r}
install.packages("lqmm")
```

```{r}

library(lqmm)

```

```{r}

# Assuming you have:
# - `data`: Your dataset.
# - `property`: A dependent variable.
# - `earlyLifeTrt`: Early life treatment factor.
# - `adultTrt`: Adult treatment factor.
# - `mouseID`: Mouse ID factor.
# - `cellID`: Cell ID factor.

# Let's create a list to store models for different quantiles
quantiles <- c(0.25, 0.5, 0.75)
models <- list()

# for (tau in quantiles) {
#  models[[as.character(tau)]] <- lqmm(
#     fixed = relPeak ~ earlyLifeTrt * adultTrt, 
#     random = ~ 1 | mouseID/cellID,
#     tau = tau, 
#     data = data
#   )
# 
# # Let's examine the summaries for each quantile
# for (tau in names(models)) {
#   cat("Summary for tau =", tau, "quantile:\n")
#   print(summary(models[[tau]]))
#   cat("\n\n")
# }

for (tau in quantiles) {
  models[[as.character(tau)]] <- lqmm(
    fixed = relPeak ~ earlyLifeTrt * adultTrt, 
    random = ~ 1 | mouseID/cellID,
    tau = tau, 
    data = pscProps
  )
}

# Let's examine the summaries for each quantile
# This loop is separate from the one above and therefore needs its own opening curly brace.
for (tau in names(models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(models[[tau]]))
  cat("\n\n")
}
```


```{r}
# Fit models for specified quantiles
for (tau in quantiles) {
  model <- lqmm(
    fixed = relPeak ~ earlyLifeTrt * adultTrt, 
    random = ~ 1 | mouseID/cellID,
    tau = tau, 
    data = pscProps
  )
  models[[as.character(tau)]] <- model
}

# Examine the summaries for each quantile
for (tau in names(models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(models[[tau]]))
  cat("\n\n")
}
```


```{r}
model <- lqmm(
  fixed = relPeak ~ earlyLifeTrt * adultTrt,
  random = ~ 1 | mouseID,
  tau = 0.50,
  data = pscProps
)

```

```{r}
model <- lqmm(
  fixed = relPeak ~ 1,  # A simple intercept-only model
  random = ~ 1 | mouseID,
  tau = 0.50,
  data = pscProps
)

summary(model)
```

```{r}
install.packages("lqmm")
```


```{r}
library(lqmm)
# set.seed(123)  # Set seed for reproducibility

# Create sample data
num_mice <- 30  # Let's assume we have 30 mice
num_measurements <- 5  # Each mouse has 5 measurements

# Generate a data frame
sample_data <- expand.grid(
  mouseID = factor(1:num_mice),       # Mouse ID factor
  measurementID = 1:num_measurements  # Measurement within each mouse
)

# Simulate a response variable with some random noise and a treatment effect
sample_data$response <- 10 +  # Some baseline response level
                         ifelse(sample_data$mouseID %% 2 == 0, 3, -2) +  # Treatment A or B effect
                         rnorm(nrow(sample_data), mean = 0, sd = 1)  # Add some noise

# Simulate a treatment factor with two levels
# (e.g., even mouse IDs receive treatment A, odd receive treatment B)
sample_data$treatment <- ifelse(sample_data$mouseID %% 2 == 0, "TreatmentA", "TreatmentB")
sample_data$treatment <- as.factor(sample_data$treatment)  # Convert to factor

# Simple lqmm with random intercept for each mouse
model <- lqmm(
  fixed = response ~ treatment,
  random = ~ 1 | mouseID,
  tau = 0.5,  # Fitting at the median (0.5 quantile)
  data = sample_data
)

# View the summary of the model
summary(model)
```

```{r}
model <- lqmm(fixed = response ~ treatment, tau = 0.5, data = sample_data)
```


```{r}
# We will also assume that pscProps$relPeak, pscProps$earlyLifeTrt, and pscProps$adultTrt are appropriately formatted.
# Convert mouseID to character as shown in the Orthodont example, if it's not already:
pscProps$mouseID <- as.character(pscProps$mouseID)
pscProps$cellID <- as.character(pscProps$cellID)

# Fit a simple quantile regression with random intercept for mouseID
# Note: 'group' instead of 'groups', no nesting with '/', and mouseID as a character
fit_lqmm <- lqmm(
  fixed = relPeak ~ earlyLifeTrt * adultTrt,
  random = ~ 1,
  group = mouseID,
  tau = 0.5,  # median
  data = pscProps
)

# Check the summary
summary(fit_lqmm)
```

```{r}
# Generate sample data
sample_size <- 100  # Define the total number of observations

# Create a data frame with a binary treatment variable and a random mouseID
sample_data <- data.frame(
  mouseID = as.character(rep(1:10, each = sample_size / 10)), # 10 mice, each with 10 measurements
  treatment = sample(rep(c("Control", "Treatment"), each = sample_size / 2)), # Binary treatment
  response = rnorm(sample_size, mean = 100, sd = 10) # Response with normal distribution
)

# Add an effect of treatment and random noise by mouseID
sample_data$response[sample_data$treatment == "Treatment"] <- sample_data$response[sample_data$treatment == "Treatment"] + 20
sample_data$response <- sample_data$response + rnorm(n = 10, mean = 0, sd = 5)[as.numeric(sample_data$mouseID)]

# Convert 'treatment' to a factor
sample_data$treatment <- as.factor(sample_data$treatment)

# Fit a quantile regression model using lqmm
fit_sample_lqmm <- lqmm(
  fixed = response ~ treatment,
  random = ~ 1,
  group = mouseID,
  tau = 0.5,  # median
  data = sample_data
)

# Check the summary of the model
summary(fit_sample_lqmm)
```


```{r}
str(pscProps)
```

```{r}
summary(pscProps$relPeak)
```


```{r}
levels(pscProps$earlyLifeTrt)
levels(pscProps$adultTrt)
```

```{r}
sum(is.na(pscProps$relPeak))
sum(is.nan(pscProps$relPeak))
sum(is.infinite(pscProps$relPeak))

any(is.na(pscProps$mouseID))
any(is.na(pscProps$earlyLifeTrt))
any(is.na(pscProps$adultTrt))
```

```{r}
table(pscProps$mouseID)
```

```{r}
head(pscProps, n = 20)  # Show the first 20 rows of the data
```


```{r}
# Convert 'mouseID' to a factor
pscProps$mouseID <- as.factor(pscProps$mouseID)

# Fit a basic quantile regression model with random intercepts for 'mouseID'
fit_basic_lqmm <- lqmm(
  fixed = relPeak ~ earlyLifeTrt + adultTrt,
  random = ~ 1,
  group = mouseID,
  tau = 0.25,  # median
  data = pscProps
)

# Check the summary of the model
summary(fit_basic_lqmm)
```


```{r}
subset_pscProps <- pscProps[pscProps$mouseID %in% c("701", "702", "713", "714"),]

# Fit the model on the subset
fit_basic_lqmm_subset <- lqmm(
  fixed = relPeak ~ earlyLifeTrt + adultTrt,
  random = ~ 1,
  group = mouseID,
  tau = 0.5,
  data = subset_pscProps
)

pscProps %>%
  group_by(
    mouseID
    , earlyLifeTrt
    , adultTrt
  ) %>%
  summarize(
    n()
  )
```

This one works with fwhm. It doesn't for relPeak
```{r}
testModel <- lqmm(fwhm ~ earlyLifeTrt * adultTrt
     , random = ~1
     , group = cellID
     , tau = 0.5
     , data = pscProps
   )
```
This one works with fwhm. It doesn't for relPeak
```{r}
testModel <- lqmm(fwhm ~ earlyLifeTrt * adultTrt
     , random = ~ 1
     , group = cellID
     , tau = 0.5
     , data = pscProps
   )
```

# Full width half max - Quantile analysis - works

```{r}
# Let's create a list to store models for different quantiles
quantiles <- c(
  0.25
  , 0.5
  , 0.75
)
models <- list()

for (tau in quantiles) {
  models[[as.character(tau)]] <- lqmm(
    fwhm ~ earlyLifeTrt * adultTrt
    , random = ~ 1
    , group = cellID
    , tau = tau
    , data = pscProps
    , control = lqmmControl(
      LP_max_iter = 2000
    ) 
   )
  
}

# Let's examine the summaries for each quantile
for (tau in names(models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(models[[tau]]))
  cat("\n\n")
}
```

```{r}
fwhm_quartile <- pscProps %>%
  group_by(
    cellID
    , mouseID
    , damID
    , earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    fwhm
  )

anova_test(
  fwhm_quartile
  , q1 ~ earlyLifeTrt * adultTrt
  , type = 3
)

mixed(
  q1 ~ earlyLifeTrt * adultTrt + (1|mouseID) + (1|damID)
  , fwhm_quartile
  , method = "KR"
)
```
```{r}
relPeak_quartile <- pscProps %>%
  group_by(
    cellID
    , mouseID
    , damID
    , earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    relPeak
  )

anova_test(
  relPeak_quartile
  , q1 ~ earlyLifeTrt * adultTrt
  , type = 3
)

mixed(
  q1 ~ earlyLifeTrt * adultTrt + (1|mouseID) + (1|damID)
  , relPeak_quartile
  , method = "KR"
)
anova_test(
  relPeak_quartile
  , median ~ earlyLifeTrt * adultTrt
  , type = 3
)

mixed(
  median ~ earlyLifeTrt * adultTrt + (1|mouseID) + (1|damID)
  , relPeak_quartile
  , method = "KR"
)
anova_test(
  relPeak_quartile
  , q3 ~ earlyLifeTrt * adultTrt
  , type = 3
)

mixed(
  q3 ~ earlyLifeTrt * adultTrt + (1|mouseID) + (1|damID)
  , relPeak_quartile
  , method = "KR"
)
```


# Rise time - Quantile analysis - works

```{r}
# Let's create a list to store models for different quantiles
quantiles <- c(
  0.25
  , 0.5
  , 0.75
)
models <- list()

for (tau in quantiles) {
  models[[as.character(tau)]] <- lqmm(
    riseTime ~ earlyLifeTrt * adultTrt
    , random = ~ 1
    , group = cellID
    , tau = tau
    , data = pscProps
    , control = lqmmControl(
      LP_max_iter = 2000
    ) 
   )
  
}

# Let's examine the summaries for each quantile
for (tau in names(models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(models[[tau]]))
  cat("\n\n")
}
```

# Decay time - Quantile analysis - doesn't work

```{r}
# Let's create a list to store models for different quantiles
quantiles <- c(
  0.25
  , 0.5
  , 0.75
)
models <- list()

for (tau in quantiles) {
  models[[as.character(tau)]] <- lqmm(
    decay9010 ~ earlyLifeTrt * adultTrt
    , random = ~ 1
    , group = cellID
    , tau = tau
    , data = pscProps
    , control = lqmmControl(
      LP_max_iter = 2000
    )
   )
  
}

# Let's examine the summaries for each quantile
for (tau in names(models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(models[[tau]]))
  cat("\n\n")
}
```
# Relative peak - quantile analysis - doesn't work

```{r}
# Let's create a list to store models for different quantiles
quantiles <- c(
  0.25
  , 0.5
  , 0.75
)
models <- list()

for (tau in quantiles) {
  models[[as.character(tau)]] <- lqmm(
    relPeak ~ earlyLifeTrt * adultTrt
    , random = ~ 1
    , group = cellID
    , tau = tau
    , data = pscProps
    , control = lqmmControl(
      LP_max_iter = 2000
    ) 
   )
  
}

# Let's examine the summaries for each quantile
for (tau in names(models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(models[[tau]]))
  cat("\n\n")
}
```

Remove the lowest 1% and highest 1% of data
```{r}
lower_bound <- quantile(pscProps$relPeak, probs = 0.2)
upper_bound <- quantile(pscProps$relPeak, probs = 0.8)

pscProps_outliersRemoved <- pscProps %>%
  filter(
    relPeak > lower_bound & relPeak < upper_bound
  )

relPeak_models <- list()

for (tau in quantiles) {
  relPeak_models[[as.character(tau)]] <- lqmm(
    relPeak ~ earlyLifeTrt * adultTrt
    , random = ~ 1
    , group = cellID
    , tau = tau
    , data = pscProps_outliersRemoved
    , control = lqmmControl(
      LP_max_iter = 2000
    ) 
  )
  
}

# Summaries for each quantile
for (tau in names(relPeak_models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(relPeak_models[[tau]]))
  cat("\n\n")
}
```

```{r}
summary(lqmm(
  fixed = fwhm ~ earlyLifeTrt * adultTrt
  , random = ~ 1
  , group = cellID
  , tau = c(.25, .5, .75)
  , data = pscProps
  , control = lqmmControl(
    LP_max_iter = 2000
  )
))
```


```{r}
# Base R plot
qqnorm(pscProps$relPeak)
qqline(pscProps$relPeak) # Adds a trend line

# ggplot2
library(ggplot2)

ggplot(pscProps, aes(sample = relPeak)) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of relPeak") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles") +
  theme_minimal()
```



# kSamples distributions

```{r}
install.packages("kSamples")
```

```{r}
library(kSamples)
```

## 4-way Anderson-Darling

```{r}
group_STD_CON <- pscProps$relPeak[pscProps$earlyLifeTrt == "STD" & pscProps$adultTrt == "CON"]
group_STD_ALPS <- pscProps$relPeak[pscProps$earlyLifeTrt == "STD" & pscProps$adultTrt == "ALPS"]
group_LBN_CON <- pscProps$relPeak[pscProps$earlyLifeTrt == "LBN" & pscProps$adultTrt == "CON"]
group_LBN_ALPS <- pscProps$relPeak[pscProps$earlyLifeTrt == "LBN" & pscProps$adultTrt == "ALPS"]

# Perform the Anderson-Darling test to compare the four distributions
ad_test_result <- ad.test(group_STD_CON, group_STD_ALPS, group_LBN_CON, group_LBN_ALPS)

# Output the results
print(ad_test_result)
```

### Pairwise

```{r}
groups_list <- list(
  "STD-CON" = group_STD_CON
  , "STD-ALPS" = group_STD_ALPS
  , "LBN-CON" = group_LBN_CON
  , "LBN-ALPS" = group_LBN_ALPS
)
# Pairwise Anderson-Darling tests
pairs <- combn(length(groups_list), 2, simplify = FALSE)
pairwise_results <- lapply(pairs, function(p) {
  list(groups = p, 
       test_result = ad.test(groups_list[[p[1]]], groups_list[[p[2]]]))
})

# Print the pairwise test results
for (result in pairwise_results) {
  cat("Comparing Group", result$groups[1], "and Group", result$groups[2], ":\n")
  print(result$test_result)
  cat("\n")
}
```

```{r}
getAD_Pval <- function(result, version = 1, fromPaired = FALSE){
  if(fromPaired){
    result <- result$test_result
  }
  tbl <- result$ad
  tbl_tibble <- tbl %>% as.tibble()
  p_val <- tbl_tibble$` asympt. P-value`[[version]]
  return(p_val)
}
```


### Adjusted
```{r}
# Assuming 'pairwise_results' contains the results of your pairwise Anderson-Darling tests
# Extracting p-values from the test results
p_values <- sapply(pairwise_results, function(result){
  p_val <- getAD_Pval(result, fromPaired = TRUE)
  return(p_val)
})

# Apply the Holm correction to the p-values
p_adjusted <- p.adjust(p_values, method = "holm")

# Pairwise comparison labels
pair_labels <- sapply(pairs, function(p) paste("Group", p[1], "vs Group", p[2]))

# Combine the labels and adjusted p-values into a data frame for a clearer presentation
comparison_results <- data.frame(
  Pair = pair_labels,
  P_Value = p_values,
  P_Adjusted = p_adjusted
)

# Print the results with Holm correction
print(comparison_results)
```


Missing the pairwise


```{r}
# Bootstrap undersampling function
undersample_data <- function(data, max_per_cell = 15) {
  # Split data by cellID
  data_split <- split(data, data$cellID)
  
  # Sample up to max_per_cell observations per cellID
  data_sampled <- do.call("rbind", lapply(data_split, function(events) {
    if(nrow(events) > max_per_cell) {
      events[sample(1:nrow(events), max_per_cell), ]
    } else {
      events
    }
  }))
  
  return(data_sampled)
}

# Initialize list to store ad.test results from each bootstrap iteration
bootstrap_results <- vector("list", 500)

# Perform the ad.test 200 times using undersampled data
for (i in 1:500) {
  # Create an undersampled dataset
  undersampled_data <- undersample_data(pscProps)
  
  # Calculate the Anderson-Darling test from the undersampled data
  group_STD_CON <- undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "STD" & undersampled_data$adultTrt == "CON"]
  group_STD_ALPS <- undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "STD" & undersampled_data$adultTrt == "ALPS"]
  group_LBN_CON <- undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "LBN" & undersampled_data$adultTrt == "CON"]
  group_LBN_ALPS <- undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "LBN" & undersampled_data$adultTrt == "ALPS"]
  
  # Perform the 4-way Anderson-Darling test
  ad_test_result <- kSamples::ad.test(group_STD_CON, group_STD_ALPS, group_LBN_CON, group_LBN_ALPS)
  
  # Store results in list
  bootstrap_results[[i]] <- ad_test_result
}

# Extract p-values from each result and apply the Holm correction
bootstrap_p_values <- sapply(bootstrap_results, function(result){
  p_val <- getAD_Pval(result)
  return(p_val)
})

average_p_value <- format(round(mean(bootstrap_p_values), 3), nsmall = 3)

print(paste("Average p-value across bootstrap replications:", average_p_value))
```

```{r}
pscProps %>%
  group_by(
    cellID
    , earlyLifeTrt
    , adultTrt
  ) %>%
  summarize(
    nEvents = n()
    , .groups = "drop"
  ) %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    nEvents
  )
```


Saving the mean vectors - doesn't work

```{r}
# Extended bootstrap undersampling function to return list of vectors
undersample_data_groups <- function(data, max_per_cell = 15) {
  # Split data by cellID
  data_split <- split(data, data$cellID)
  
  # Sample up to max_per_cell observations per cellID
  data_sampled <- do.call("rbind", lapply(data_split, function(events) {
    if(nrow(events) > max_per_cell) {
      events[sample(1:nrow(events), max_per_cell), ]
    } else {
      events
    }
  }))

  # Split into groups after undersampling
  group_vectors <- list(
    STD_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "CON"],
    STD_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "ALPS"],
    LBN_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "CON"],
    LBN_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "ALPS"]
  )
  
  return(group_vectors)
}

# Create matrices to store accumulated values for each group
group_STD_CON_accum <- matrix(nrow=500, ncol=15)
group_STD_ALPS_accum <- matrix(nrow=500, ncol=15)
group_LBN_CON_accum <- matrix(nrow=500, ncol=15)
group_LBN_ALPS_accum <- matrix(nrow=500, ncol=15)

# Perform the ad.test 500 times using undersampled data
for (i in 1:500) {
  # Get the undersampled group vectors
  groups <- undersample_data_groups(pscProps)
  
  # Accumulate the values
  group_STD_CON_accum[i, ] <- groups$STD_CON
  group_STD_ALPS_accum[i, ] <- groups$STD_ALPS
  group_LBN_CON_accum[i, ] <- groups$LBN_CON
  group_LBN_ALPS_accum[i, ] <- groups$LBN_ALPS
}

# Function to calculate the mean across each column (observation) in the matrices
mean_across_iterations <- function(matrix) {
  colMeans(matrix, na.rm=TRUE)
}

# Calculate the average vectors across all bootstrap iterations
average_vector_STD_CON <- mean_across_iterations(group_STD_CON_accum)
average_vector_STD_ALPS <- mean_across_iterations(group_STD_ALPS_accum)
average_vector_LBN_CON <- mean_across_iterations(group_LBN_CON_accum)
average_vector_LBN_ALPS <- mean_across_iterations(group_LBN_ALPS_accum)

# Perform a final Anderson-Darling test using the average vectors
final_ad_test_result <- kSamples::ad.test(
  average_vector_STD_CON,
  average_vector_STD_ALPS,
  average_vector_LBN_CON,
  average_vector_LBN_ALPS
)

# Output the final test result
print(final_ad_test_result)
```

This only gives one value for each group
```{r}
# Extended bootstrap undersampling function to return list of vectors
undersample_data_groups <- function(data, max_per_cell = 15) {
  # Split data by cellID
  data_split <- split(data, data$cellID)
  
  # Sample up to max_per_cell observations per cellID
  data_sampled <- do.call("rbind", lapply(data_split, function(events) {
    if(nrow(events) > max_per_cell) {
      events[sample(1:nrow(events), max_per_cell), ]
    } else {
      events
    }
  }))

  # Split into groups after undersampling
  group_vectors <- list(
    STD_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "CON"],
    STD_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "ALPS"],
    LBN_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "CON"],
    LBN_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "ALPS"]
  )
  
  return(group_vectors)
}

# Function to compile all values into a list while doing the bootstrap
accumulate_samples <- function(group_list, group_samples) {
  for (group_name in names(group_list)) {
    group_list[[group_name]] <- c(group_list[[group_name]], group_samples[[group_name]])
  }
  return(group_list)
}

# Initialize lists to accumulate values
group_samples_list <- list(
  STD_CON = c(),
  STD_ALPS = c(),
  LBN_CON = c(),
  LBN_ALPS = c()
)

# Perform the ad.test 500 times using undersampled data
for (i in 1:500) {
  # Get the undersampled group vectors
  groups <- undersample_data_groups(pscProps)
  
  # Accumulate sample values across all bootstrap iterations
  group_samples_list <- accumulate_samples(group_samples_list, groups)
}

# Function to calculate the mean of vectors in a list
mean_of_vector_list <- function(vector_list) {
  sapply(vector_list, function(x) mean(x, na.rm = TRUE))
}

# Calculate the average vector for each group
average_vectors <- mean_of_vector_list(group_samples_list)

# Conduct the Anderson-Darling test
final_ad_test_result <- ad.test(
  x = list(
    average_vectors[['STD_CON']],
    average_vectors[['STD_ALPS']],
    average_vectors[['LBN_CON']],
    average_vectors[['LBN_ALPS']]
  )
)

# Output the final test result
print(final_ad_test_result)
```

```{r}
# Extended bootstrap undersampling function to return list of vectors
undersample_data_groups <- function(data, max_per_cell = 15) {
  # Split data by cellID
  data_split <- split(data, data$cellID)
  
  # Sample up to max_per_cell observations per cellID
  data_sampled <- do.call("rbind", lapply(data_split, function(events) {
    if(nrow(events) > max_per_cell) {
      events[sample(1:nrow(events), max_per_cell), ]
    } else {
      events
    }
  }))

  # Split into groups after undersampling
  group_vectors <- list(
    STD_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "CON"],
    STD_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "ALPS"],
    LBN_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "CON"],
    LBN_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "ALPS"]
  )
  
  return(group_vectors)
}

# Function to accumulate samples in a list of vectors for each group
accumulate_samples <- function(accumulated_samples, new_samples) {
  lapply(names(accumulated_samples), function(group) {
    list(c(accumulated_samples[[group]], new_samples[[group]]))
  })
}

# Initialize lists to accumulate values for each group (one list per group)
accumulated_samples_list <- list(
  STD_CON = vector("list", 500),
  STD_ALPS = vector("list", 500),
  LBN_CON = vector("list", 500),
  LBN_ALPS = vector("list", 500)
)

# Perform the bootstrap 500 times using undersampled data
for (i in 1:500) {
  groups <- undersample_data_groups(pscProps) # Your existing function
  
  # Accumulate the samples across all bootstrap iterations for each group
  accumulated_samples_list <- lapply(names(accumulated_samples_list), function(group) {
    accumulated_samples_list[[group]][[i]] <- groups[[group]]
    accumulated_samples_list[[group]]
  })
}

accumulated_samples_list[["STD_CON"]]

# Now, accumulated_samples_list contains a list of vectors for the distribution of relPeak for each group

# To calculate average vectors for each group
average_vectors <- lapply(accumulated_samples_list, function(group_list) {
  mean_values <- sapply(group_list, mean, na.rm = TRUE)
  mean(mean_values, na.rm = TRUE)
})

# Now average_vectors contains an average of means for each bootstrap sample per treatment group.
# The average_vectors will look like this: list(STD_CON = X.XX, STD_ALPS = X.XX, ...)
print(average_vectors[["STD_CON"]])
```






```{r}
# Bootstrap undersampling function (same as before)
undersample_data <- function(data, max_per_cell = 20) {
  # ... (Function definition remains the same as previously provided)
}

# Function for performing all pairwise Anderson-Darling tests
pairwise_ad_test <- function(groups_list) {
  pairs <- combn(names(groups_list), 2, simplify = FALSE)
  pairwise_results <- lapply(pairs, function(p) {
    list(groups = p, 
         test_result = ad.test(groups_list[[p[1]]], groups_list[[p[2]]]))
  })
  
  # Extract p-values
  p_values <- sapply(pairwise_results, function(result) result$test_result$p.value)
  
  # Holm correction
  p_adjusted <- p.adjust(p_values, method = "holm")
  names(p_adjusted) <- sapply(pairs, function(p) paste(p[1], "vs", p[2]))
  
  return(p_adjusted)
}

# Store p-value matrices for each bootstrap iteration
p_value_matrices <- vector("list", 200)

# Perform 200 bootstrap replications
for (i in 1:200) {
  # Create an undersampled dataset
  undersampled_data <- undersample_data(pscProps)
  
  # Split into groups
  groups_list <- list(
    STD_CON = undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "STD" & undersampled_data$adultTrt == "CON"],
    STD_ALPS = undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "STD" & undersampled_data$adultTrt == "ALPS"],
    LBN_CON = undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "LBN" & undersampled_data$adultTrt == "CON"],
    LBN_ALPS = undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "LBN" & undersampled_data$adultTrt == "ALPS"]
  )
  
  # Perform pairwise Anderson-Darling tests with Holm correction
  p_value_matrices[[i]] <- pairwise_ad_test(groups_list)
}

# Average the adjusted p-values across all pairwise comparisons and bootstrap replications
average_pvalues <- Reduce("+", p_value_matrices) / length(p_value_matrices)

# Display the averaged adjusted p-values
print(average_pvalues)
```



# Getting an average vector for each group and then running the anderson-darling test on this

```{r}
average_sorted_vectors <- function(vector_list, sortDec = FALSE) {
  # Check if all vectors are of the same length
  lengths <- sapply(vector_list, length)
  unique_lengths <- unique(lengths)
  
  if(length(unique_lengths) != 1) {
    stop("Not all vectors are of the same length.")
  }
  
  # Sort each vector individually
  sorted_vectors <- lapply(vector_list, function(x) sort(x, decreasing = sortDec))
  
  # Calculate the average for each position
  average_vector <- Reduce("+", sorted_vectors) / length(sorted_vectors)
  
  # Return the average vector
  return(average_vector)
}

# Extended bootstrap undersampling function to return list of vectors
undersample_data_groups <- function(data, max_per_cell = 15) {
  # Split data by cellID
  data_split <- split(data, data$cellID)
  
  # Sample up to max_per_cell observations per cellID
  data_sampled <- do.call("rbind", lapply(data_split, function(events) {
    if(nrow(events) > max_per_cell) {
      events[sample(1:nrow(events), max_per_cell), ]
    } else {
      events
    }
  }))

  # Split into groups after undersampling
  group_vectors <- list(
    STD_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "CON"],
    STD_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "ALPS"],
    LBN_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "CON"],
    LBN_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "ALPS"]
  )
  
  return(group_vectors)
}

nIterations <- 50

STD_CON_vals = vector("list", nIterations)
STD_ALPS_vals = vector("list", nIterations)
LBN_CON_vals = vector("list", nIterations)
LBN_ALPS_vals = vector("list", nIterations)

# Perform the ad.test 500 times using undersampled data
for (i in 1:nIterations) {
  # Get the undersampled group vectors
  groups <- undersample_data_groups(pscProps)
  
  # Accumulate sample values across all bootstrap iterations
  STD_CON_vals[[i]] = groups$STD_CON
  STD_ALPS_vals[[i]] = groups$STD_ALPS
  LBN_CON_vals[[i]] = groups$LBN_CON
  LBN_ALPS_vals[[i]] = groups$LBN_ALPS
}

# doesn't ultimately matter, but sorted descending since this is relPeak
STD_CON_avg <- average_sorted_vectors(STD_CON_vals, sortDec = TRUE)
STD_ALPS_avg <- average_sorted_vectors(STD_ALPS_vals, sortDec = TRUE)
LBN_CON_avg <- average_sorted_vectors(LBN_CON_vals, sortDec = TRUE)
LBN_ALPS_avg <- average_sorted_vectors(LBN_ALPS_vals, sortDec = TRUE)


# Conduct the Anderson-Darling test
final_ad_test_result <- ad.test(
  x = list(
    STD_CON_avg
    , STD_ALPS_avg
    , LBN_CON_avg
    , LBN_ALPS_avg
  )
)

# Output the final test result
print(final_ad_test_result)
```

```{r}
# Convert average vectors to data frames
data_STD_CON <- data.frame(
  Value = STD_CON_avg
  , earlyLifeTrt = "STD"
  , adultTrt = "CON"
)
data_STD_ALPS <- data.frame(
  Value = STD_ALPS_avg
  , earlyLifeTrt = "STD"
  , adultTrt = "ALPS"
)
data_LBN_CON <- data.frame(
  Value = LBN_CON_avg
  , earlyLifeTrt = "LBN"
  , adultTrt = "CON"
)
data_LBN_ALPS <- data.frame(
  Value = LBN_ALPS_avg
  , earlyLifeTrt = "LBN"
  , adultTrt = "ALPS"
)

# Calculate ECDF for each group within the data
calc_ecdf <- function(data){
  data <- data %>%
    arrange(Value) %>%
    mutate(CumProb = ecdf(-Value)(-Value)
    #        ) %>%
    # mutate(Value = -Value # Flip the values for reverse plotting
           )  
  return(data)
}

# Apply the function to each data frame separately
data_STD_CON <- calc_ecdf(data_STD_CON)
data_STD_ALPS <- calc_ecdf(data_STD_ALPS)
data_LBN_CON <- calc_ecdf(data_LBN_CON)
data_LBN_ALPS <- calc_ecdf(data_LBN_ALPS)

# Combine all data frames into one
cdf_data <- rbind(data_STD_CON, data_STD_ALPS, data_LBN_CON, data_LBN_ALPS) %>%
  combineStress()

# Plot using ggplot2
plot_max15 <- ggplot(cdf_data, aes(x = Value, y = CumProb, color = comboTrt)) +
  geom_step() +
  scale_x_reverse() +
  labs(
    x = "relative peak (pA)", 
    y = "Cumulative Probability", 
    color = "Treatment"
  ) +
  boxTheme() + 
  textTheme(size = 16) +
  comboTrtLineColor() +
  coord_cartesian(xlim = c(0, -100))

plot_max100
plot_max15
```
```{r}
group_vectors <- list(
    STD_CON = pscProps$relPeak[pscProps$earlyLifeTrt == "STD" & pscProps$adultTrt == "CON"],
    STD_ALPS = pscProps$relPeak[pscProps$earlyLifeTrt == "STD" & pscProps$adultTrt == "ALPS"],
    LBN_CON = pscProps$relPeak[pscProps$earlyLifeTrt == "LBN" & pscProps$adultTrt == "CON"],
    LBN_ALPS = pscProps$relPeak[pscProps$earlyLifeTrt == "LBN" & pscProps$adultTrt == "ALPS"]
  )


# Convert average vectors to data frames
data_STD_CON <- data.frame(
  Value = group_vectors[["STD_CON"]]
  , earlyLifeTrt = "STD"
  , adultTrt = "CON"
)
data_STD_ALPS <- data.frame(
  Value = group_vectors[["STD_ALPS"]]
  , earlyLifeTrt = "STD"
  , adultTrt = "ALPS"
)
data_LBN_CON <- data.frame(
  Value = group_vectors[["LBN_CON"]]
  , earlyLifeTrt = "LBN"
  , adultTrt = "CON"
)
data_LBN_ALPS <- data.frame(
  Value = group_vectors[["LBN_ALPS"]]
  , earlyLifeTrt = "LBN"
  , adultTrt = "ALPS"
)

# Calculate ECDF for each group within the data
calc_ecdf <- function(data){
  data <- data %>%
    arrange(Value) %>%
    mutate(CumProb = ecdf(-Value)(-Value)
    #        ) %>%
    # mutate(Value = -Value # Flip the values for reverse plotting
           )  
  return(data)
}

# Apply the function to each data frame separately
data_STD_CON <- calc_ecdf(data_STD_CON)
data_STD_ALPS <- calc_ecdf(data_STD_ALPS)
data_LBN_CON <- calc_ecdf(data_LBN_CON)
data_LBN_ALPS <- calc_ecdf(data_LBN_ALPS)

# Combine all data frames into one
cdf_data <- rbind(data_STD_CON, data_STD_ALPS, data_LBN_CON, data_LBN_ALPS) %>%
  combineStress()

# Plot using ggplot2
ggplot(cdf_data, aes(x = Value, y = CumProb, color = comboTrt)) +
  geom_step() +
  scale_x_reverse() +
  labs(
    x = "relative peak (pA)", 
    y = "Cumulative Probability", 
    color = "Treatment"
  ) +
  boxTheme() + 
  textTheme(size = 16) +
  comboTrtLineColor() +
  coord_cartesian(xlim = c(0, -100))
```


```{r}


# Example usage:
vec1 <- c(30, 20, 50, 40)
vec2 <- c(25, 25, 15, 45)
vec3 <- c(35, 15, 25, 55) # Added a third vector for demonstration

# Combine the vectors into a list
list_of_vectors <- list(vec1, vec2, vec3)

# Call the average_sorted_vectors function
result_vector <- average_sorted_vectors(list_of_vectors)
result_vector
```

```{r}
emmeans(
  logriseTime_models
  , "adultTrt"
  , by = "earlyLifeTrt"
)

logriseTime_models$y


```

```{r}
lqmm(
  log10(riseTime) ~ earlyLifeTrt * adultTrt
  , random = ~ 1
  , group = cellID
  , tau = quantiles
  , data = pscProps
  , control = lqmmControl(
    LP_max_iter = 2000
  )
)
```
```{r}
logrelPeak_models_sum <- summary(logrelPeak_models)
```
# Summary 

```{r}
# ultimately need to set a seed before running these, I think, as it changes with each run
logrelPeak_models_sum <- summary(logrelPeak_models)
logdecay9010_models_sum <- summary(logdecay9010_models)
logFWHM_models_sum <- summary(logFWHM_models)
logriseTime_models_sum <- summary(logriseTime_models)
```

```{r}
logrelPeak_models_sum
logdecay9010_models_sum
logFWHM_models_sum
logriseTime_models_sum
```


```{r}
group_vectors <- list(
    STD_CON = pscProps$relPeak[pscProps$earlyLifeTrt == "STD" & pscProps$adultTrt == "CON"],
    STD_ALPS = pscProps$relPeak[pscProps$earlyLifeTrt == "STD" & pscProps$adultTrt == "ALPS"],
    LBN_CON = pscProps$relPeak[pscProps$earlyLifeTrt == "LBN" & pscProps$adultTrt == "CON"],
    LBN_ALPS = pscProps$relPeak[pscProps$earlyLifeTrt == "LBN" & pscProps$adultTrt == "ALPS"]
  )


# Convert average vectors to data frames
data_STD_CON <- data.frame(
  Value = group_vectors[["STD_CON"]]
  , earlyLifeTrt = "STD"
  , adultTrt = "CON"
)
data_STD_ALPS <- data.frame(
  Value = group_vectors[["STD_ALPS"]]
  , earlyLifeTrt = "STD"
  , adultTrt = "ALPS"
)
data_LBN_CON <- data.frame(
  Value = group_vectors[["LBN_CON"]]
  , earlyLifeTrt = "LBN"
  , adultTrt = "CON"
)
data_LBN_ALPS <- data.frame(
  Value = group_vectors[["LBN_ALPS"]]
  , earlyLifeTrt = "LBN"
  , adultTrt = "ALPS"
)

# Calculate ECDF for each group within the data
calc_ecdf <- function(data){
  data <- data %>%
    arrange(Value) %>%
    mutate(CumProb = ecdf(-Value)(-Value))  
  return(data)
}

# Apply the function to each data frame separately
data_STD_CON <- calc_ecdf(data_STD_CON)
data_STD_ALPS <- calc_ecdf(data_STD_ALPS)
data_LBN_CON <- calc_ecdf(data_LBN_CON)
data_LBN_ALPS <- calc_ecdf(data_LBN_ALPS)

# Combine all data frames into one
cdf_data <- rbind(data_STD_CON, data_STD_ALPS, data_LBN_CON, data_LBN_ALPS) %>%
  combineStress()

# Plot using ggplot2
ggplot(cdf_data, aes(x = Value, y = CumProb, color = comboTrt)) +
  geom_step() +
  scale_x_reverse() +
  labs(
    x = "relative peak (pA)", 
    y = "Cumulative Probability", 
    color = "Treatment"
  ) +
  boxTheme() + 
  textTheme(size = 16) +
  comboTrtLineColor() +
  coord_cartesian(xlim = c(0, -100))
```

```{r}
pscProps_log <- pscProps %>%
  mutate(
    logPeak = -log(-relPeak)
    , .after = relPeak
  ) %>%
  combineStress()

pscProps_log %>%
  arrange(- relPeak)
```

```{r}

pscProps_log %>%
  ggplot(aes(x = logPeak, color = comboTrt)) +
  stat_ecdf() +
  comboTrtLineColor() +
  coord_cartesian(
    xlim = c(-4.2, -3.8)
    , ylim = c(.2, .3)
  )
```

```{r}
# Create a new data frame for the prediction
new_data <- data.frame(
  earlyLifeTrt = factor(c("STD", "STD", "LBN", "LBN"), levels = c("STD", "LBN")),
  adultTrt = factor(c("CON", "ALPS", "CON", "ALPS"), levels = c("CON", "ALPS")),
  `(Intercept)` = 1
) %>%
  combineStress() %>%
  rename(
    `earlyLifeTrtLBN` = earlyLifeTrt
    ,`adultTrtALPS` = adultTrt
    ,`earlyLifeTrtLBN:adultTrtALPS` = comboTrt
  )

predict(logrelPeak_models, newdata = new_data)

pscProps %>%
  filter(
    earlyLifeTrt == "STD"
    , adultTrt == "ALPS"
  ) %>%
  group_by(
    cellID
  )

# Predict the conditional quantile for the combination of treatments
predicted_value <- predict(logrelPeak_models$)

# See the predicted value
predicted_value
```

```{r}
set_default_contrasts()
## Orthodont data
data(Orthodont)

# Random intercept model
fitOi.lqmm <- lqmm(distance ~ age, random = ~ 1, group = Subject,
	tau = c(0.1,0.5,0.9), data = Orthodont)

# Predict (y - Xb)	
predict(fitOi.lqmm, level = 0)

# Predict (y - Xb - Zu)
predict(fitOi.lqmm, level = 1)

# 95% confidence intervals
predint(fitOi.lqmm, level = 0, alpha = 0.05)

```

```{r}
set_default_contrasts()
## Orthodont data
data(Orthodont)

df <- Orthodont %>%
  as.data.frame() %>%
  ungroup

# Random intercept model
fitOi.lqmm <- lqmm(distance ~ age, random = ~ 1, group = Subject,
	tau = c(0.1,0.5,0.9), data = Orthodont)
fitOi.lqmm.df <- lqmm(distance ~ age, random = ~ 1, group = Subject,
	tau = c(0.1,0.5,0.9), data = df)

fitOi.lqmm
fitOi.lqmm.df

# Numeric variable seems to give changing predictions
fit.test <- lqmm(
  riseTime ~ cort_hr5
  , random = ~1 
  , group = cellID
  , tau = c(0.25, 0.5, 0.75)
  , data = pscProps %>%
    left_join(
      acuteStressFiltered
      , by = "mouseID"
    )
)

predict(fit.test) %>%
  as.data.frame()

fit.test <- lqmm(
  riseTime ~ earlyLifeTrt
  , random = ~1 
  , group = cellID
  , tau = c(0.25, 0.5, 0.75)
  , data = pscProps %>%
    mutate(
      earlyLifeTrtVar = as.numeric(earlyLifeTrt)
      , .before = "earlyLifeTrt"
    )
)

predict(fit.test) %>%
  as.data.frame()

fitOi.sex.lqmm <- lqmm(distance ~ age * Sex, random = ~ 1, group = Subject,
	tau = c(0.1,0.5,0.9), data = df)

fitOi.sex.lqmm

predict(fitOi.sex.lqmm, level = 0)

Orthodont

predict(logDecayTime_models, level = 0)

# Predict (y - Xb)	
predict(fitOi.lqmm, level = 0)
# Predict (y - Xb)	
predict(fitOi.lqmm.df, level = 0)

# Predict (y - Xb - Zu)
predict(fitOi.lqmm, level = 1)

# 95% confidence intervals
predint(fitOi.lqmm, level = 0, alpha = 0.05)

```

```{r}
pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = comboTrt,
      y = relPeak,
      # color = cellID
      # , fill = cellID
      color = -log(-relPeak)
      , fill = -log(-relPeak)
    )
  ) +
  jitterGeom(
    size = .5,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  expand_limits(y=0)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.title.x = element_blank()
  ) + 
  labs(y = "-log(-amplitude (pA))") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis()
```
```{r}
pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = comboTrt,
      y = -log(-relPeak),
      # color = cellID
      # , fill = cellID
      color = -log(-relPeak)
      , fill = -log(-relPeak)
    )
  ) +
  jitterGeom(
    size = .5,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  expand_limits(y=0)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.title.x = element_blank()
  ) + 
  labs(y = "-log(-amplitude (pA))") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis()
```
```{r}
pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = cellID,
      y = -log(-relPeak),
      color = -log(-relPeak)
      , fill = -log(-relPeak)
    )
  ) +
  jitterGeom(
    size = 1,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  expand_limits(y=0)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.title.x = element_blank()
    , axis.text.x = element_blank()
  ) + 
  labs(y = "-log(-amplitude (pA))") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  )
```
```{r}
# Define the custom transformation
neg_log_trans <- trans_new(
  name = "neg_log",
  transform = function(y) -log10(-y),
  inverse = function(y) -10^(-y)
)

pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = cellID,
      y = relPeak,
      color = -log(-relPeak)
      , fill = -log(-relPeak)
    )
  ) +
  jitterGeom(
    size = 1,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  expand_limits(y=-1)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.title.x = element_blank()
    , axis.text.x = element_blank()
  ) + 
  labs(y = "-log(-amplitude (pA))") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  ) +
  scale_y_continuous(trans = neg_log_trans, labels = scales::math_format())
```
```{r}
pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = cellID,
      y = relPeak,
      color = -log(-relPeak)
      , fill = -log(-relPeak)
    )
  ) +
  jitterGeom(
    size = 1,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  expand_limits(y=-1)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.title.x = element_blank()
    , axis.text.x = element_blank()
  ) + 
  labs(y = "-log(-amplitude (pA))") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  ) +
  scale_y_continuous(trans = neg_log_trans, breaks = trans_breaks("neg_log", function(x) -10^(-x)), labels = trans_format("neg_log", math_format(-10^.x)))
```
```{r}
# Define the custom transformation
neg_log_trans <- trans_new(
  name = "neg_log"
  , transform = function(y) -log10(-y)
  , inverse = function(y) -10^(-y)
  # , breaks = c(-1, -10, -50, -100, -150, -200, -250, -300, -350, -400)
  , domain = c(-Inf, -0.0001)
)

df_counts <- pscProps %>%
  group_by(cellID) %>%
    summarize(
      count = n()
      , .groups = "drop"
    ) %>%
    arrange(
      desc(count)
    )

pscProps %>%
  mutate(
    cellID = factor(
      cellID
      , levels = df_counts$cellID
    )
  ) %>%
  combineStress() %>%
  ggplot(
    aes(
      x = cellID,
      y = relPeak,
      color = -log(-relPeak)
      , fill = -log(-relPeak)
    )
  ) +
  jitterGeom(
    size = .75,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  # expand_limits(y=-1)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    # axis.title.x = element_blank()
    axis.text.x = element_blank()
  ) + 
  labs(y = "amplitude (pA)", x = "cell") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  ) +
  scale_y_continuous(
    trans = neg_log_trans
    , breaks = c(-2.5, -5, -10, -20, -40, -80, -160, -320, -480)
  )
```

```{r}
pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = comboTrt,
      y = relPeak,
      color = cellID
      , fill = cellID
    )
  ) +
  jitterGeom(
    size = .75,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  # expand_limits(y=0)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.title.x = element_blank()
  ) + 
  labs(y = "amplitude (pA)") +
  scale_color_viridis(discrete = TRUE) +
  scale_fill_viridis(discrete = TRUE) +
  scale_y_continuous(
    trans = neg_log_trans
    , breaks = c(-2.5, -5, -10, -20, -40, -80, -160, -320, -480)
  )
```

```{r}
pscProps %>%
  mutate(
    cellID = factor(
      cellID
      , levels = df_counts$cellID
    )
  ) %>%
  combineStress() %>%
  ggplot(
    aes(
      x = cellID,
      y = riseTime,
      color = log(riseTime)
      , fill = log(riseTime)
    )
  ) +
  jitterGeom(
    size = .75,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    # axis.title.x = element_blank()
    axis.text.x = element_blank()
  ) + 
  labs(y = "rise time (ms)", x = "cell") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  # ) +
  # scale_y_log10(
  #   breaks = c(0.00125, 0.0025, 0.005, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12)
  #   , labels = c("0.00125", "0.0025", "0.005", "0.01", "0.02", "0.04", "0.08", "0.16", "0.32", "0.64", "1.28", "2.56", "5.12")
  )
```
```{r}
pscProps %>%
  mutate(
    cellID = factor(
      cellID
      , levels = df_counts$cellID
    )
  ) %>%
  combineStress() %>%
  ggplot(
    aes(
      x = cellID,
      y = riseTime,
      color = log(riseTime)
      , fill = log(riseTime)
    )
  ) +
  jitterGeom(
    size = .75,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    # axis.title.x = element_blank()
    axis.text.x = element_blank()
  ) + 
  labs(y = "rise time (ms)", x = "cell") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  ) +
  scale_y_log10(
    breaks = c(0.00125, 0.0025, 0.005, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12)
    , labels = c("0.00125", "0.0025", "0.005", "0.01", "0.02", "0.04", "0.08", "0.16", "0.32", "0.64", "1.28", "2.56", "5.12")
  )
```

```{r}
pscProps %>%
  filter(
    riseTime < 0.01
  )
pscProps %>%
  filter(
    riseTime < 0.1
  )
```

# Add Quartile Colors Function
```{r}
add_quartile_colors <- function(
    data
    , group_col
    , value_col
    , reverse_order = FALSE
) {
  data <- data %>%
    group_by({{ group_col }}) %>%
    mutate(
      QuartileID = cut({{ value_col }},
                       breaks = quantile({{ value_col }}, probs = 0:4/4, na.rm = TRUE),
                       include.lowest = TRUE,
                       labels = FALSE)
    ) %>% 
    ungroup()
  
  # quartile_colors <- viridis(4, option = "D")
  quartile_colors <- c("1" = "#2c7bb6", "2" = "#abd9e9", "3" = "#fdae61", "4" = "#d7191c")
  if (reverse_order) {
    color_assignment <- (4 - as.numeric(data$QuartileID)) +1
  } else {
    color_assignment <- as.numeric(data$QuartileID)
  }

  data$Color <- quartile_colors[color_assignment]

  data <- data %>%
    select(-QuartileID)

    # mutate(
    #   Color = viridis(4, option = "D")[as.numeric(QuartileID)]
    # ) %>%
    # select(-QuartileID)

  return(data)
}

pscProps %>%
  add_quartile_colors(
    cellID
    , relPeak
    , reverse_order = TRUE
  )
```



# Plotting function - log

```{r}
plotPSCProp_log <- function(
  df
  , yVar
  , yLab
  , logBreaks
  , logLabels
  , byCell = TRUE
  , dotSize = 0.75
  , dotAlpha = 0.6
  , jitterWidth = 0.35
  , byQuartiles = TRUE
  , sortByQuartile = TRUE
){
  if(byCell){
    if(sortByQuartile){
      df_counts <- df %>%
        group_by(cellID) %>%
          summarize(
            quartile25 = quantile({{ yVar }}, probs = 0.25, na.rm = TRUE)
            , .groups = "drop"
          ) %>%
          arrange(
            desc(quartile25)
          )
      
    } else {
      df_counts <- df %>%
        group_by(cellID) %>%
          summarize(
            count = n()
            , .groups = "drop"
          ) %>%
          arrange(
            desc(count)
          )
      
    }
    
    df <- df %>%
      mutate(
        cellID = factor(
          cellID
          , levels = df_counts$cellID
        )
      )
  }
  
  df <- df %>%
    combineStress()
  
  if(byQuartiles){
    df <- df %>%
      add_quartile_colors(
        if(byCell){cellID} else {comboTrt}
        , value_col = {{ yVar }}
      )
  }
  
  viz <- df %>%
    ggplot(
      aes(
        x = if(byCell){cellID} else{comboTrt}
        , y = {{ yVar }}
        , color = if(byQuartiles){Color} else {log({{ yVar }})}
        , fill = if(byQuartiles){Color} else {log({{ yVar }})}
      )
  ) +
  jitterGeom(
    size = dotSize,
    alpha = dotAlpha,
    width = jitterWidth,
    height = 0
  ) +
  theme_pubr()+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.text.x = element_blank()
  ) + 
  labs(y = yLab, x = ifelse(byCell, "cell", ""))
  
  if(byQuartiles){
    viz <- viz +
      scale_color_identity() +
      scale_fill_identity()
  } else {
    viz <- viz +
      scale_color_viridis() +
      scale_fill_viridis()
  }
  
  viz <- viz +
    facet_wrap(
      ~ comboTrt
      , nrow = 1
      , scales = "free_x"
      , strip.position = "bottom"
    ) +
    scale_y_log10(
      breaks = logBreaks
      , labels = logLabels
    )
  
  return(viz)
}
```

# Plotting function - neg log - quartiles

```{r}
plotPSCProp_negLog_quartiles <- function(
  df
  , yVar
  , yLab
  , logBreaks = c(-2.5, -5, -10, -20, -40, -80, -160, -320, -480)
  , logLabels = c("-2.5", "-5", "-10", "-20", "-40", "-80", "-160", "-320", "-480")
  , byCell = TRUE
  , dotSize = 0.75
  , dotAlpha = 0.6
  , jitterWidth = 0.35
){
  # Define the custom transformation
  neg_log_trans <- trans_new(
    name = "neg_log"
    , transform = function(y) -log10(-y)
    , inverse = function(y) -10^(-y)
    , domain = c(-Inf, -0.0001)
  )
  
  if(byCell){
    df_counts <- df %>%
      group_by(cellID) %>%
        summarize(
          count = n()
          , .groups = "drop"
        ) %>%
        arrange(
          desc(count)
        )
    
    df <- df %>%
      mutate(
        cellID = factor(
          cellID
          , levels = df_counts$cellID
        )
      )
  }
  
  df <- df %>%
    combineStress() %>%
    add_quartile_colors(
      if(byCell){cellID} else {comboTrt}
      , value_col = {{ yVar }}
    )
  
  viz <- df %>%
    ggplot(
      aes(
        x = if(byCell){cellID} else{comboTrt}
        , y = {{ yVar }}
        , color = Color
        , fill = Color
      )
  ) +
  jitterGeom(
    size = dotSize,
    alpha = dotAlpha,
    width = jitterWidth,
    height = 0
  ) +
  theme_pubr()+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.text.x = element_blank()
  ) + 
  labs(y = yLab, x = ifelse(byCell, "cell", "")) +
  scale_color_identity() +
  scale_fill_identity() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  ) +
  scale_y_continuous(
    trans = neg_log_trans
    , breaks = logBreaks
    , labels = logLabels
  )
  
  return(viz)
}
```

# Plotting function - neg log

```{r}
plotPSCProp_negLog <- function(
  df
  , yVar
  , yLab
  , logBreaks = c(-2.5, -5, -10, -20, -40, -80, -160, -320, -480)
  , logLabels = c("-2.5", "-5", "-10", "-20", "-40", "-80", "-160", "-320", "-480")
  , byCell = TRUE
  , dotSize = 0.75
  , dotAlpha = 0.6
  , jitterWidth = 0.35
  , byQuartiles = TRUE
  , reverseColor = FALSE
  , sortByQuartile = TRUE
){
  # Define the custom transformation
  neg_log_trans <- trans_new(
    name = "neg_log"
    , transform = function(y) -log10(-y)
    , inverse = function(y) -10^(-y)
    , domain = c(-Inf, -0.0001)
  )
  
  if(byCell){
    if(sortByQuartile){
      df_counts <- df %>%
        group_by(cellID) %>%
          summarize(
            quartile25 = quantile({{ yVar }}, probs = 0.25, na.rm = TRUE)
            , .groups = "drop"
          ) %>%
          arrange(
            desc(quartile25)
          )
      
    } else {
      df_counts <- df %>%
        group_by(cellID) %>%
          summarize(
            count = n()
            , .groups = "drop"
          ) %>%
          arrange(
            desc(count)
          )
      
    }
    
    df <- df %>%
      mutate(
        cellID = factor(
          cellID
          , levels = df_counts$cellID
        )
      )
  }
  
  df <- df %>%
    combineStress()
  
  if(byQuartiles){
    df <- df %>%
      add_quartile_colors(
        if(byCell){cellID} else {comboTrt}
        , value_col = {{ yVar }}
        , reverse_order = reverseColor
      )
  }
  
  viz <- df %>%
    ggplot(
      aes(
        x = if(byCell){cellID} else{comboTrt}
        , y = {{ yVar }}
        , color = if(byQuartiles){Color} else {- log(- {{ yVar }})}
        , fill = if(byQuartiles){Color} else {- log(- {{ yVar }})}
      )
  ) +
  jitterGeom(
    size = dotSize,
    alpha = dotAlpha,
    width = jitterWidth,
    height = 0
  ) +
  theme_pubr()+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.text.x = element_blank()
  ) + 
  labs(y = yLab, x = ifelse(byCell, "cell", ""))
  
  if(byQuartiles){
    viz <- viz +
      scale_color_identity() +
      scale_fill_identity()
  } else {
    viz <- viz +
      scale_color_viridis() +
      scale_fill_viridis()
  }
  
  viz <- viz +
    facet_wrap(
      ~ comboTrt
      , nrow = 1
      , scales = "free_x"
      , strip.position = "bottom"
    ) +
    scale_y_continuous(
      trans = neg_log_trans
      , breaks = logBreaks
      , labels = logLabels
    )
  
  return(viz)
}
```


```{r}
pscProps <- pscProps %>%
  mutate(
    absAmp = -relPeak
  )
```

# AMPLITUDE

## By cell

Be careful with reversing color, if the percentiles for the model are not reversed

```{r}
relPeak_byCell <- pscProps %>%
  # plotPSCProp_negLog(
  #   yVar = relPeak
  #   , yLab = "amplitude (pA)"
  #   , byCell = TRUE
  #   , reverseColor = FALSE
  # )
  plotPSCProp_log(
    yVar = amplitude
    , yLab = "amplitude (pA)"
    , logBreaks = c(5, 10, 20, 40, 80, 160, 320, 480)
    , logLabels = c("5", "10", "20", "40", "80", "160", "320", "480")
    , byCell = TRUE
  )

relPeak_byCell
```

## By treatment

```{r}
relPeak_byTrt <- pscProps %>%
  # plotPSCProp_negLog(
  #   yVar = relPeak
  #   , yLab = "amplitude (pA)"
  #   , byCell = FALSE
  #   , reverseColor = FALSE
  # )
  plotPSCProp_log(
    yVar = absAmp
    , yLab = "amplitude (pA)"
    , logBreaks = c(5, 10, 20, 40, 80, 160, 320, 480)
    , logLabels = c("5", "10", "20", "40", "80", "160", "320", "480")
    , byCell = FALSE
  )

relPeak_byTrt
```

# RISE TIME

## By Cell

```{r}
riseTime_byCell <- pscProps %>%
  plotPSCProp_log(
    yVar = riseTime
    , yLab = "rise time (ms)"
    , logBreaks = c(0.00125, 0.0025, 0.005, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12)
    , logLabels = c("0.00125", "0.0025", "0.005", "0.01", "0.02", "0.04", "0.08", "0.16", "0.32", "0.64", "1.28", "2.56", "5.12")
    , byCell = TRUE
  )

riseTime_byCell
```

## By treatment
```{r}
riseTime_byTrt <- pscProps %>%
  plotPSCProp_log(
    yVar = riseTime
    , yLab = "rise time (ms)"
    , logBreaks = c(0.00125, 0.0025, 0.005, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12)
    , logLabels = c("0.00125", "0.0025", "0.005", "0.01", "0.02", "0.04", "0.08", "0.16", "0.32", "0.64", "1.28", "2.56", "5.12")
    , byCell = FALSE
  )
riseTime_byTrt
```

# DECAY TIME

## By Cell

```{r}
decayTime_byCell <- pscProps %>%
  plotPSCProp_log(
    yVar = decay9010
    , yLab = "decay time (ms)"
    , logBreaks = c(0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24, 20.48, 40.96, 81.92, 163.84)
    , logLabels = c("0.08", "0.16", "0.32", "0.64", "1.28", "2.56", "5.12", "10.24", "20.48", "40.96", "81.92", "163.84")
    , byCell = TRUE
  )
decayTime_byCell
```

## By treatment
```{r}
decayTime_byTrt <- pscProps %>%
  plotPSCProp_log(
    yVar = decay9010
    , yLab = "decay time (ms)"
    , logBreaks = c(0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24, 20.48, 40.96, 81.92, 163.84)
    , logLabels = c("0.08", "0.16", "0.32", "0.64", "1.28", "2.56", "5.12", "10.24", "20.48", "40.96", "81.92", "163.84")
    , byCell = FALSE
  )
decayTime_byTrt
```
# FWHM

## By Cell

```{r}
fwhm_byCell <- pscProps %>%
  plotPSCProp_log(
    yVar = fwhm
    , yLab = "fwhm (ms)"
    , logBreaks = c(0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24, 20.48, 40.96)
    , logLabels = c("0.16", "0.32", "0.64", "1.28", "2.56", "5.12", "10.24", "20.48", "40.96")
    , byCell = TRUE
  ) +
  expand_limits(y = 0.16)

fwhm_byCell
```

## By treatment
```{r}
fwhm_byTrt <- pscProps %>%
  plotPSCProp_log(
    yVar = fwhm
    , yLab = "fwhm (ms)"
    , logBreaks = c(0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24, 20.48, 40.96)
    , logLabels = c("0.16", "0.32", "0.64", "1.28", "2.56", "5.12", "10.24", "20.48", "40.96")
    , byCell = FALSE
  ) +
  expand_limits(y = 0.16)

fwhm_byTrt
```

```{r}
pscProps %>%
  filter(
    relPeak > -5
  )
```


# Set-up
```{r}
figureNum <- 1

pptBaseName <- makeBaseNameFunc("")

plotFolder <- file.path(plotOutputFolder, "GABA_PSC_props")

imgType <-"png"

exportImg <- exportImg_forPurposeFunc(
  imgType = imgType
  , figNumFunc = pptBaseName
  , plotFolder = plotFolder
  , compType = currentCompType
)

exportFullPPTSlide <- function(
    plot = last_plot()
    , baseName = fileBaseName
    , figNum = figureNum
    , width = 11.5
    , height = 5
){
  exportImg(
    plot = plot
    , fileBaseName = baseName
    , figNum = figNum
    , units = "in"
    , width = width
    , height = height
  )
}

exportHalfPPTSlide <- function(
    plot = last_plot()
    , baseName = fileBaseName
    , figNum = figureNum
){
  exportImg(
    plot = plot
    , fileBaseName = baseName
    , figNum = figNum
    , units = "in"
    , width = 5.67
    , height = 5
  )
}

exportThirdPPTSlide <- function(
    plot = last_plot()
    , baseName = fileBaseName
    , figNum = figureNum
){
  exportImg(
    plot = plot
    , fileBaseName = baseName
    , figNum = figNum
    , units = "in"
    , width = 3.6
    , height = 5
  )
}

exportQuarterPPTSlide <- function(
    plot = last_plot()
    , baseName = fileBaseName
    , figNum = figureNum
){
  exportImg(
    plot = plot
    , fileBaseName = baseName
    , figNum = figNum
    , units = "in"
    , width = 3.2
    , height = 5
  )
}

editableImgs <- TRUE
pptAddOneGraph <- function(
    title = slideTitle
    , plot = last_plot()
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_oneGraph(
    officer_ppt, 
    title, 
    plot, 
    makeEditable = editableImgs
  )
  return(officer_ppt)
}

pptAddOneTable <- function(
    table
    , title = slideTitle
    , officer_ppt = ppt
    , dontFormat = TRUE
){
  officer_ppt <- addSlide_oneTable(
    officer_ppt, 
    title, 
    table, 
    dontFormat = dontFormat
  )
  return(officer_ppt)
}

pptUneditAddOneGraph <- function(
    title = slideTitle
    , plot = last_plot()
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_oneGraph(
    officer_ppt, 
    title, 
    plot, 
    makeEditable = FALSE
  )
  return(officer_ppt)
}

pptAddTwoGraphs <- function(
    plot1
    , plot2
    , title = slideTitle
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_twoGraph(
    officer_ppt, 
    title, 
    plot1, 
    plot2,
    makeEditable = editableImgs
  )
  return(officer_ppt)
}

pptAddTwoGraphsMoreLeft <- function(
    plot1
    , plot2
    , title = slideTitle
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_twoGraphMoreLeft(
    officer_ppt, 
    title, 
    plot1, 
    plot2,
    makeEditable = editableImgs
  )
  return(officer_ppt)
}

pptAddGraphTable <- function(
    plot
    , table
    , title = slideTitle
    , officer_ppt = ppt
    , dontFormat = TRUE
){
  officer_ppt <- addSlide_graphTable(
    officer_ppt, 
    title, 
    plot, 
    table,
    makeEditable = editableImgs
    , dontFormat = dontFormat
    , textSize = textSize
  )
  return(officer_ppt)
}

pptAddFourGraphs <- function(
    plot1
    , plot2
    , plot3
    , plot4
    , title = slideTitle
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_fourGraph(
    officer_ppt, 
    title, 
    plot1, 
    plot2, 
    plot3,
    plot4,
    makeEditable = editableImgs
    )
  return(officer_ppt)
}

pptAddThreeGraphs <- function(
    plot1
    , plot2
    , plot3
    , title = slideTitle
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_threeGraph(
    officer_ppt, 
    title, 
    plot1, 
    plot2, 
    plot3,
    makeEditable = editableImgs
    )
  return(officer_ppt)
}

pptAddStats <- function(
    statsText
    , officer_ppt = ppt
){
  officer_ppt <- addStatsToBottom(
    officer_ppt,
    statsText
  )
}

pptAddTwoStats <- function(
    statsText1
    , statsText2
    , officer_ppt = ppt
){
  officer_ppt <- addTwoStatsToGraphs(
    officer_ppt,
    statsText1,
    statsText2
  )
}

pptAddSectionHead <- function(
    title = slideTitle
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_sectionHead(
    officer_ppt
    , title
  )
}



```

```{r}
# make powerpoint and title slide
ppt <- read_pptx("genericPPT.pptx")
ppt <- add_slide(ppt, layout = "Title Slide")

# layout_properties(ppt, layout="Title Slide")

subText <- "PSC properties"
ppt <- ph_with(
  ppt,
  value = subText,
  location = ph_location_label("Subtitle 2")
)


# Run this function to save the PPT file to the disk
# On windows, don't run if the PPT file is open
savePPT <- makeSavePPTFunc(
  presPPT = ppt
  , presFolder = plotFolder
  , presFileName = "AGG_LBN_PSC_props"
  , addDate = TRUE
)
```

```{r}
relPeak_byCell
fileBaseName <- "relPeak_byCell"
slideTitle <- "Amplitude"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
relPeak_byTrt
fileBaseName <- "relPeak_byTrt"
slideTitle <- "Amplitude"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
riseTime_byCell
fileBaseName <- "riseTime_byCell"
slideTitle <- "Rise time"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
riseTime_byTrt
fileBaseName <- "riseTime_byTrt"
slideTitle <- "Rise time"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
decayTime_byCell
fileBaseName <- "decayTime_byCell"
slideTitle <- "Decay time"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
decayTime_byTrt
fileBaseName <- "decayTime_byTrt"
slideTitle <- "Decay time"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
fwhm_byCell
fileBaseName <- "fwhm_byCell"
slideTitle <- "FWHM"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
fwhm_byTrt
fileBaseName <- "fwhm_byTrt"
slideTitle <- "FWHM"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```

```{r}
logAmplitude_models_sum$tTable[[1]]

rownames(logAmplitude_models_sum$tTable[[1]])

logAmplitude_models_sum$tTable[[1]]
  
simplifyQuartileOutput <- function(df # just one quartile
                                   ){
  df %>%
    subEarlyLifeTrtInRowNames_quartile() %>%
    subAdultTrtInRowNames_quartile() %>%
    subColonInRowNames() %>%
    as.data.frame() %>%
    rownames_to_column(var = "fixed effect") %>%
    rename(
      SEM = `Std. Error`
      , p = `Pr(>|t|)`
    ) %>%
    mutate(
      Value = 10^Value
      , SEM = 10^SEM
      , `95% CI` = paste0(
        "[", format(
          round(
            10^`lower bound`
            , 2
          )
          , nsmall = 2, trim = TRUE
        ), ", ", format(
          round(
            10^`upper bound`
            , 2
          )
          , nsmall = 2, trim = TRUE
        ), "]")
      , .after = SEM
    ) %>% 
    select(
      -c(`lower bound`, `upper bound`)
    ) %>%
    formatPCol()
}

simplifyAllQuartilesOutput <- function(allDFs){
  
  resultDF <- NULL
  
  for(name in names(allDFs)){
    df <- allDFs[[name]]
    
    simpDF <- simplifyQuartileOutput(df) %>%
      mutate(
        quartile = name
        , .before = `fixed effect`
      )
    
     resultDF <- bind_rows(resultDF, simpDF)
  }
  
  return(resultDF)
}

logAmplitude_models_sum$tTable %>%
  simplifyAllQuartilesOutput()


names(logAmplitude_models_sum$tTable)

for(name in names(logAmplitude_models_sum$tTable)){
  print(name)
}

logAmplitude_models_sum$tTable[["0.25"]]

logAmplitude_models_sum$tTable[[]] %>%
  simplifyAllQuartilesOutput()
  simplifyQuartileOutput()

pscProps$earlyLifeTrt <- factor(pscProps$earlyLifeTrt, levels = c("STD", "LBN"))

bind_rows(
  list(
    "0.25" = logAmplitude_models_sum$tTable[[1]] %>% as.tibble()
    , "0.5" = logAmplitude_models_sum$tTable[[2]] %>% as.tibble()
    , "0.75" = logAmplitude_models_sum$tTable[[3]] %>% as.tibble()
  )
  , .id = "percentile"
)
```



```{r}
names(logModels_tbl)
```
Something odd about the Q3 estimate for FWHM, way too large. Not sure why, but there are also some differences in the Covariance matrix of the random effects
```{r}
pscProps_log <- pscProps %>%
  mutate(
    logFWHM = log10(fwhm)
    , logRiseTime = log10(riseTime)
    , logDecayTime = log10(decay9010)
    , logAmplitude = log10(amplitude)
  )

pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    logFWHM
  )
pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    logRiseTime
  )
pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    logDecayTime
  )
pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    logAmplitude
  )

pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    fwhm
  )
pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    riseTime
  )
pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    decay9010
  )
pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    amplitude
  )

lqmm(
  logFWHM ~ earlyLifeTrt * adultTrt
  , random = ~1
  , group = cellID
  , tau = quantiles
  , data = pscProps_log
  , control = lqmmControl(
    LP_max_iter = 2500
  )
)
pscProps_log
```

```{r}
predict(logFWHM_models) %>%
  as.data.frame()
```

```{r}
getLQMMpredictions <- function(
    lqmmMod
    , df = pscProps
    , responseVals = TRUE # transforms from log10
){
  # meanPredict <- predict(
  #   lqmmMod
  #   , level = 0 # the average across the random effects. If level = 1, gives unique value for each subject group
  # ) %>%
  #   as.data.frame() %>%
  #   bind_cols(
  #     df %>%
  #       select(
  #         cellID
  #         , mouseID
  #         , earlyLifeTrt
  #         , adultTrt
  #       )
  #   ) %>%
  #   group_by(
  #     earlyLifeTrt
  #     , adultTrt
  #   ) %>%
  #   summarize(
  #     "q0.25" = mean(`0.25`, na.rm = TRUE)
  #     , "q0.5" = mean(`0.50`, na.rm = TRUE)
  #     , "q0.75" = mean(`0.75`, na.rm = TRUE)
  #     , .groups = "drop"
  #   )
  
  intPredict <- predint(
    lqmmMod
    , level = 0
    , seed = 741
  ) %>%
    as.data.frame() %>%
    bind_cols(
      df %>%
        select(
          cellID
          , mouseID
          , earlyLifeTrt
          , adultTrt
        )
    ) %>%
    group_by(
      earlyLifeTrt
      , adultTrt
    ) %>%
    summarize(
      "mean_0.25" = mean(X0.25.yhat, na.rm = TRUE)
      , "lower_0.25" = mean(X0.25.lower, na.rm = TRUE)
      , "upper_0.25" = mean(X0.25.upper, na.rm = TRUE)
      , "SEM_0.25" = mean(X0.25.SE, na.rm = TRUE)
      , "mean_0.5" = mean(X0.50.yhat, na.rm = TRUE)
      , "lower_0.50" = mean(X0.50.lower, na.rm = TRUE)
      , "upper_0.50" = mean(X0.50.upper, na.rm = TRUE)
      , "SEM_0.50" = mean(X0.50.SE, na.rm = TRUE)
      , "mean_0.75" = mean(X0.75.yhat, na.rm = TRUE)
      , "lower_0.75" = mean(X0.75.lower, na.rm = TRUE)
      , "upper_0.75" = mean(X0.75.upper, na.rm = TRUE)
      , "SEM_0.75" = mean(X0.75.SE, na.rm = TRUE)
      , .groups = "drop"
    )
  
  if(responseVals){
    intPredict <- intPredict %>%
      mutate(
        mean_0.25 = 10^mean_0.25
        , lower_0.25 = 10^lower_0.25
        , upper_0.25 = 10^upper_0.25
        , SEM_0.25 = 10^SEM_0.25
        , mean_0.5 = 10^mean_0.5
        , lower_0.50 = 10^lower_0.50
        , upper_0.50 = 10^upper_0.50
        , SEM_0.50 = 10^SEM_0.50
        , mean_0.75 = 10^mean_0.75
        , lower_0.75 = 10^lower_0.75
        , upper_0.75 = 10^upper_0.75
        , SEM_0.75 = 10^SEM_0.75
      )
  }
  return(intPredict)
}
```


```{r}
predict(
  logFWHM_models
  , level = 0
) %>%
  as.data.frame() %>%
  bind_cols(
    pscProps %>%
      select(
        cellID
        , mouseID
        , earlyLifeTrt
        , adultTrt
      )
  ) %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  summarize(
    "q0.25" = mean(`0.25`, na.rm = TRUE)
    , "q0.5" = mean(`0.50`, na.rm = TRUE)
    , "q0.75" = mean(`0.75`, na.rm = TRUE)
  )
predict(
  logFWHM_models
  , level = 1
) %>%
  as.data.frame() %>%
  bind_cols(
    pscProps %>%
      select(
        cellID
        , mouseID
        , earlyLifeTrt
        , adultTrt
      )
  ) %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  summarize(
    "q0.25" = mean(`0.25`, na.rm = TRUE)
    , "q0.5" = mean(`0.50`, na.rm = TRUE)
    , "q0.75" = mean(`0.75`, na.rm = TRUE)
  )

predint(
  logFWHM_models
  , level = 0
  , seed = 741
) %>%
  as.data.frame() %>%
  bind_cols(
    pscProps %>%
      select(
        cellID
        , mouseID
        , earlyLifeTrt
        , adultTrt
      )
  ) %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  )

```

```{r}
logAmplitude_models %>%
  getLQMMpredictions(responseVals = FALSE)
```


```{r}
pscProps_log %>%
  ggplot(aes(x = logPeak, color = comboTrt)) +
  stat_ecdf() +
  comboTrtLineColor() +
  coord_cartesian(
    xlim = c(-4.2, -3.8)
    , ylim = c(.2, .3)
  )
```

```{r}
set_sum_contrasts()

amplitude_effectCodedMod<- lqmm(
  log10(amplitude) ~ earlyLifeTrt * adultTrt
  , random = ~ 1
  , group = cellID
  , tau = quantiles
  , data = pscProps
  , control = lqmmControl(
    LP_max_iter = 1000
    , LP_tol_ll = 5e-5
    , LP_tol_theta = 5e-5
  )
)
```

```{r}
amplitude_effectCodedMod_sum <- summary(
  amplitude_effectCodedMod
  , seed = 741
)
```

```{r}
amplitude_effectCodedMod_sum
```



```{r}
amplitude_effectCodedMod %>%
  getLQMMpredictions(
    responseVals = FALSE
  )

ampPred <- predict(
  amplitude_effectCodedMod
) %>%
  as.data.frame()

ampPredInt <- predint(
  amplitude_effectCodedMod
) %>%
  as.data.frame()

ampPredInt_741 <- predint(
  amplitude_effectCodedMod
  , seed = 741
) %>%
  as.data.frame()

ampPredInt_231 <- predint(
  amplitude_effectCodedMod
  , seed = 231
) %>%
  as.data.frame()

unique(ampPred$`0.25`)
unique(ampPred$`0.50`)
unique(ampPred$`0.75`)

unique(ampPredInt$X0.75.yhat)

ampPredInt %>%
  bind_cols(
    pscProps
  ) %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  meanSummary(
    X0.75.yhat
  )
ampPredInt_741 %>%
  bind_cols(
    pscProps
  ) %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  meanSummary(
    X0.75.yhat
  )
ampPredInt_231 %>%
  bind_cols(
    pscProps
  ) %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  meanSummary(
    X0.75.yhat
  )
```

```{r}
pscProps %>%
  mutate(
    logAmp = log10(amplitude)
  ) %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(c(logAmp))
```



```{r}

amplitude_effectCodedMod %>%
  getLQMMpredictions(
    responseVals = FALSE
  )

logAmplitude_models %>%
  getLQMMpredictions(
    responseVals = FALSE
  )

amplitude_effectCodedMod %>%
  getLQMMpredictions(
    responseVals = TRUE
  )

logAmplitude_models %>%
  getLQMMpredictions(
    responseVals = TRUE
  )
```

```{r}
amplitude_effectCodedMod$theta_x
amplitude_effectCodedMod$theta_z

amplitude_effectCodedMod$InitialPar$theta

pscProps %>%
  mutate(
    logAmp = log10(amplitude)
  ) %>%
  quartilesSummary(logAmp)
pscProps %>%
  mutate(
    logAmp = log10(amplitude)
  ) %>%
  meanSummary(logAmp)
```

```{r}
logFWHM_models_sum$`0.75`$opt
```

```{r}
logAmplitude_models_sum$`0.75`


logFWHM_models_startQR <- lqmm(
  log10(fwhm) ~ earlyLifeTrt * adultTrt
  , random = ~1
  , group = cellID
  , tau = c(0.25, 0.5, 0.75)
  , data = pscProps
  , control = lqmmControl(
    # check_theta = TRUE
    # , LP_tol_ll = 1e-6
    # , reset_step = TRUE
    # , UP_max_iter = 100
    startQR = TRUE
  )
)

logFWHM_models_startQR_sum <- logFWHM_models_startQR %>%
  summary(
    seed = 231
    , R = 100
  )

logFWHM_models_startQR_sum
```


```{r}
ggplot(pscProps, aes(sample = log10(amplitude))) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of log amplitude") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles") +
  theme_minimal()
ggplot(pscProps, aes(sample = log10(riseTime))) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of log rise time") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles") +
  theme_minimal()
ggplot(pscProps, aes(sample = log10(decay9010))) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of log decay time") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles") +
  theme_minimal()
ggplot(pscProps, aes(sample = log10(fwhm))) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of log fwhm") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles") +
  theme_minimal()

ggplot(pscProps, aes(sample = amplitude)) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of amplitude") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles") +
  theme_minimal()
ggplot(pscProps, aes(sample = riseTime)) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of rise time") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles") +
  theme_minimal()
ggplot(pscProps, aes(sample = decay9010)) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of decay time") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles") +
  theme_minimal()
ggplot(pscProps, aes(sample = fwhm)) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of fwhm") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles") +
  theme_minimal()
```


```{r}

scaledAmplitude <- scale(pscProps$amplitude)

attributes(scaledAmplitude)
pscProps_scaled <- pscProps %>%
  mutate(
    scaledAmplitude = scaledAmplitude
  )

ggplot(pscProps, aes(sample = scale(log10(amplitude)))) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of scaled amplitude") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles") +
  theme_minimal()
```

# Number of events

```{r}
pscProps %>%
  group_by(
    cellID
    , earlyLifeTrt
    , adultTrt
  ) %>%
  summarize(
    numEvents = n()
    , .groups = "drop"
  ) %>%
  combineStress() %>%
  ggplot(
    aes(
      x = numEvents
      , color = comboTrt
    )
  ) + 
  stat_ecdf() +
  boxTheme() +
  textTheme() +
  comboTrtLineColor() +
  xlab("# of events per cell") +
  ylab("proportion of cells")

pscProps %>%
  group_by(
    cellID
    , earlyLifeTrt
    , adultTrt
  ) %>%
  summarize(
    numEvents = n()
    , .groups = "drop"
  ) %>%
  combineStress() %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary()
```

```{r}
pscProps %>%
  group_by(
    cellID
  ) %>%
  arrange(
    amplitude
  ) %>%
  mutate(
    cumProbAmp = ecdf(amplitude)(amplitude)
    , .after = amplitude
  ) %>%
  arrange(
    cellID
    , amplitude
  )
```
# Relative frequency histogram

I think that this might be similar to what Caroline did in her paper (fig 1)
https://www.eneuro.org/content/5/5/ENEURO.0171-18.2018

```{r}
binWidth <- 5

binData <- pscProps %>%
  mutate(
      bin = cut(amplitude, breaks = seq(0, 400, by = binWidth), right = FALSE, labels = seq(0, 395, by = binWidth))
    , .after = amplitude
  ) %>%
  group_by(cellID, bin) %>%
  summarize(
    count = n()
    , .groups = "drop"
  )

totalCounts <- binData %>%
  group_by(cellID) %>%
  summarize(
    total = sum(count)
    , .groups = "drop"
  )

relativePropData <- binData %>%
  left_join(
    totalCounts
    , by = "cellID"
  ) %>%
  mutate(
    relProp = count/total * 100
  ) %>%
  select(
    cellID
    , bin
    , relProp
  ) %>%
  left_join(
    GABApscs_240Filtered %>%
      select(
        cellID
        , earlyLifeTrt
        , adultTrt
      )
  )

avgRelPropData <- relativePropData %>%
  group_by(earlyLifeTrt, adultTrt, bin) %>%
  summarize(
    avgRelProp = mean(relProp, na.rm = TRUE)
    , .groups = "drop"
  ) %>%
  combineStress() %>%
  mutate(
    bin = as.numeric(as.character(bin))
  )

avgRelPropData


avgRelProp_plot <- avgRelPropData %>%
  ggplot(
    aes(
      x = bin
      , y = avgRelProp
      , group = comboTrt
    )
  ) +
  geom_line(
    aes(
      color = comboTrt
    )
  ) +
  comboTrtLineColor()+
  boxTheme() +
  textTheme() +
  coord_cartesian(xlim = c(0, 120)) +
  xlab("amplitude (pA)") +
  ylab("relative frequency (%)")

avgRelProp_plot
```

```{r}
relativePropData %>%
  combineStress() %>%
  mutate(
    bin = as.numeric(as.character(bin))
  ) %>%
  ggplot(
    aes(
      x = bin
      , y = relProp
      , group = cellID
    )
  ) +
  geom_line(
    aes(
      color = comboTrt
    )
  ) +
  comboTrtLineColor()+
  boxTheme() +
  textTheme() +
  coord_cartesian(xlim = c(0, 120)) +
  xlab("amplitude (pA)") +
  ylab("relative frequency (%)")

```
This shows that the relative probabilities above, when averaged, are actually adding up to more than 100%
I wonder if this is because there are bins that aren't used, and so they're NAs for some cells, instead of zeros.
Maybe I need to have it explicitly include those zeros?
```{r}
# Continue from your previous code and calculate cumulative probabilities
cumulativePropData <- avgRelPropData %>%
  arrange(earlyLifeTrt, adultTrt, bin) %>%
  group_by(earlyLifeTrt, adultTrt) %>%
  mutate(cumProp = cumsum(avgRelProp))

# If you want the cumulative proportion to be a value between 0 and 1, you should adjust by dividing by 100
cumulativePropData <- cumulativePropData %>%
  mutate(cumPropScaled = cumProp / 100)

# Now prepare the dataset for plotting
plotData <- cumulativePropData %>%
  select(earlyLifeTrt, adultTrt, bin, cumPropScaled)

# Plot the cumulative distribution
ggplot(plotData, aes(x = bin, y = cumPropScaled, color = interaction(earlyLifeTrt, adultTrt), group = interaction(earlyLifeTrt, adultTrt))) +
  geom_step() + # geom_step for stepwise cumulative plot
  labs(x = "Event Size", y = "Cumulative Probability", title = "Cumulative Distribution Plot") +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent) # If you want the y-axis to show percentages

# Separate plots for each earlyLifeTrt and adultTrt combination, if needed
ggplot(plotData, aes(x = bin, y = cumPropScaled)) +
  geom_step() +
  facet_wrap(~interaction(earlyLifeTrt, adultTrt), scales = "free_y") + # Use facet_wrap to create a separate plot for each treatment combination
  labs(x = "Event Size", y = "Cumulative Probability", title = "Cumulative Distribution Plot by Treatment") +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent)
```
This still has the same problem.
```{r}
# Continuing from your initial binData construction...
weightedRelPropData <- binData %>%
  left_join(totalCounts, by = "cellID") %>%
  mutate(
    weightedCount = count/total
  ) %>%
  select(cellID, bin, weightedCount) %>%
  left_join(GABApscs_240Filtered %>%
    select(cellID, earlyLifeTrt, adultTrt), by="cellID")

# Calculate a weighted average relative probability
avgWeightedRelPropData <- weightedRelPropData %>%
  group_by(earlyLifeTrt, adultTrt, bin) %>%
  summarize(
    avgWeightedRelProp = sum(weightedCount) / n(), # n() is the number of cells in the group
    .groups = "drop"
  ) %>%
  mutate(
    bin = as.numeric(as.character(bin))
  )

# You can now verify that the sum of avgWeightedRelProp within each group totals to 100%
testSum <- avgWeightedRelPropData %>%
  group_by(earlyLifeTrt, adultTrt) %>%
  summarize(
    total = sum(avgWeightedRelProp),
    .groups = "drop"
  )

# If necessary, print out the total to check
testSum

# Create a dataset for a cumulative distribution plot
cumulativePlotData <- avgWeightedRelPropData %>%
  ungroup() %>%
  arrange(earlyLifeTrt, adultTrt, bin) %>%
  group_by(earlyLifeTrt, adultTrt) %>%
  mutate(cumRelProp = cumsum(avgWeightedRelProp))

# Plot the cumulative distribution
ggplot(cumulativePlotData, aes(x = bin, y = cumRelProp, group = interaction(earlyLifeTrt, adultTrt), color=interaction(earlyLifeTrt, adultTrt))) + 
  geom_step() + 
  labs(x = "Event Size", y = "Cumulative Relative Probability (%)", title = "Cumulative Distribution Plot") +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent)
```

## Corrected to include empty bins
```{r}
binWidth <- 1
maxVal <- 400

binData <- pscProps %>%
  mutate(
      bin = cut(amplitude, breaks = seq(0, maxVal, by = binWidth), right = FALSE, labels = seq(0, maxVal - binWidth, by = binWidth))
    , .after = amplitude
  ) %>%
  group_by(cellID, bin) %>%
  summarize(
    count = n()
    , .groups = "drop"
  ) %>%
  mutate(
    bin = as.numeric(as.character(bin))
  )

allBins <- seq(0, maxVal - binWidth, by = binWidth)

# need to be sure to include empty bins for all cells
binData <- binData %>%
  complete(
    cellID
    , bin = allBins
    , fill = list(count = 0)
  ) %>%
  arrange(
    cellID
    , bin
  )

totalCounts <- binData %>%
  group_by(cellID) %>%
  summarize(
    total = sum(count)
    , .groups = "drop"
  )

relativePropData <- binData %>%
  left_join(
    totalCounts
    , by = "cellID"
  ) %>%
  mutate(
    relProp = count/total * 100
  ) %>%
  select(
    cellID
    , bin
    , relProp
  ) %>%
  left_join(
    GABApscs_240Filtered %>%
      select(
        cellID
        , earlyLifeTrt
        , adultTrt
      )
  )

avgRelPropData <- relativePropData %>%
  group_by(earlyLifeTrt, adultTrt, bin) %>%
  summarize(
    avgRelProp = mean(relProp, na.rm = TRUE)
    , .groups = "drop"
  ) %>%
  combineStress()

avgRelPropData
```

### Test summation to show adds to 100
```{r}
testSum <- avgRelPropData %>%
  group_by(earlyLifeTrt, adultTrt) %>%
  summarize(
    total = sum(avgRelProp),
    .groups = "drop"
  )

testSum
```

### Plot of relative proportion
This plot looks better with larger bins (~5pA)
```{r}

avgRelProp_plot <- avgRelPropData %>%
  ggplot(
    aes(
      x = bin
      , y = avgRelProp
      , group = comboTrt
    )
  ) +
  geom_line(
    aes(
      color = comboTrt
    )
  ) +
  comboTrtLineColor()+
  boxTheme() +
  textTheme() +
  coord_cartesian(xlim = c(0, 120)) +
  xlab("amplitude (pA)") +
  ylab("relative frequency (%)")

avgRelProp_plot
```
### Cumulative distribution

This looks better with smaller bins, and with line instead of step
```{r}
# Continue from your previous code and calculate cumulative probabilities
cumulativePropData <- avgRelPropData %>%
  arrange(earlyLifeTrt, adultTrt, bin) %>%
  group_by(earlyLifeTrt, adultTrt) %>%
  mutate(cumProp = cumsum(avgRelProp)) %>%
  combineStress()

# # If you want the cumulative proportion to be a value between 0 and 1, you should adjust by dividing by 100
# cumulativePropData <- cumulativePropData %>%
#   mutate(cumPropScaled = cumProp / 100)

# Plot the cumulative distribution
cumulativePropData %>%
  ggplot(
    aes(
      x = bin
      , y = cumProp
      , color = comboTrt
      , group = comboTrt
    )
  ) +
  # geom_step() +
  geom_line() +
  labs(
    x = "amplitude (pA)"
    , y = "cumulative probability"
  ) +
  coord_cartesian(xlim = c(0, 125)) +
  boxTheme()+
  textTheme() +
  comboTrtLineColor(STD_CON_color = "grey50", LBN_CON_color = "#6dcae8") +
  theme(
    legend.position = c(0.6, 0.4)
    , legend.key = element_blank()
  )
```

# Compare to all events

```{r}
pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = amplitude
      , color = comboTrt
      , group = comboTrt
    )
  )+
  stat_ecdf(geom = "line") +
  labs(
    x = "amplitude (pA)"
    , y = "cumulative probability"
  ) +
  coord_cartesian(xlim = c(0, 125)) +
  boxTheme()+
  textTheme() +
  comboTrtLineColor(STD_CON_color = "grey50", LBN_CON_color = "#6dcae8") +
  theme(
    legend.position = c(0.6, 0.4)
    , legend.key = element_blank()
  )

# rgb_values <- col2rgb("lightblue")
# hex_color <- rgb(rgb_values[1], rgb_values[2], rgb_values[3], maxColorValue=255)
```

# Back to median by cell

```{r}
byCellMedians <- pscProps %>%
  group_by(
    cellID
    , mouseID
    , damID
    , earlyLifeTrt
    , adultTrt
  ) %>%
  summarize(
    amplitude = median(amplitude, na.rm = TRUE)
    , riseTime = median(riseTime, na.rm = TRUE)
    , decay9010 = median(decay9010, na.rm = TRUE)
    , fwhm = median(fwhm, na.rm = TRUE)
  )
  
amplitude_median_lmm <- mixed(
  amplitude ~ earlyLifeTrt * adultTrt + (1|mouseID) + (1|damID)
  , data = byCellMedians
  , method = "KR"
)
riseTime_median_lmm <- mixed(
  riseTime ~ earlyLifeTrt * adultTrt + (1|mouseID) + (1|damID)
  , data = byCellMedians
    , method = "KR"
  
)
decay9010_median_lmm <- mixed(
  decay9010 ~ earlyLifeTrt * adultTrt + (1|mouseID) + (1|damID)
  , data = byCellMedians
    , method = "KR"
)
fwhm_median_lmm <- mixed(
  fwhm ~ earlyLifeTrt * adultTrt + (1|mouseID) + (1|damID)
  , data = byCellMedians
    , method = "KR"
)

amplitude_median_lmm
riseTime_median_lmm
decay9010_median_lmm
fwhm_median_lmm


amplitude_median_errors <- amplitude_median_lmm %>%
  getErrorDF_LMM_comboTrt()
```


#MISC
```{r}
mixed(
  amplitude ~ earlyLifeTrt * adultTrt + (1|damID) + (1|mouseID) + (1|cellID)
  , pscProps
  , method = "KR"
)
```

```{r}
pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = amplitude
      , color = comboTrt
      , fill = comboTrt
    )
  ) +
  geom_density(alpha = 0.8) +
  labs(
    x = "amplitude (pA)"
    , y = "density"
  ) +
  comboTrtFillShape(STD_CON_fill = "lightgrey", STD_CON_color = "grey30")+
  theme_pubr()+
  scale_x_log10()+
  boxTheme() +
  textTheme()
```

# Bootstrapping of mean

```{r}
# Define the bootstrap function
bootstrapMeansByGroup <- function(data, columnName, groupVars, nBootstrap = 2000, replace = TRUE, setSeed = NULL) {
  # Optional: Set seed for reproducibility
  if (!is.null(setSeed)) {
    set.seed(setSeed)
  }

  # A function to resample the data and calculate the mean for the given column within each group
  bootstrapGroup <- function(dataGrouped, columnName, replace) {
    sample_data <- sample_n(dataGrouped, size = n(), replace = replace)
    mean(sample_data[[deparse(substitute(columnName))]], na.rm = TRUE)
  }
  
  # Apply bootstrapping to each group and store results
  results <- data %>%
    group_by(!!!syms(groupVars)) %>%
    summarise(BootstrapMeans = list(map_dbl(1:nBootstrap, ~bootstrapGroup(cur_data(), {{columnName}}, replace))),
              .groups = 'drop')
  
  # Return results
  return(results)
}

# Example usage:
# Assuming you have a dataframe named 'myData', and 'myVariable', 'group1' and 'group2' are your columns:
# myData <- data.frame(group1 = rep(1:2, each = 50), group2 = rep(1:2, times = 50), myVariable = rnorm(100)) 
# bootstrapResults <- bootstrapMeansByGroup(myData, myVariable, c(group1, group2), nBootstrap = 2000)
# bootstrapResults
```

```{r}
pscProps %>%
  bootstrapMeansByGroup(
    amplitude
    , exprs(earlyLifeTrt, adultTrt)
  )
```

```{r}
bootstrapGroup <- function(dataGrouped, columnName, replace) {
  sample_data <- sample_n(dataGrouped, size = n(), replace = replace)
  mean(sample_data[[deparse(substitute(columnName))]], na.rm = TRUE)
}

res <- pscProps %>%
  group_by(
    earlyLifeTrt, adultTrt
  ) %>%
  summarize(
    # bootstrapMean = list(map_dbl(1:20, ~ bootstrapGroup(pick(everything()), amplitude, replace = TRUE)))
    bootstrapMean = map_dbl(1:20, ~ bootstrapGroup(pick(everything()), amplitude, replace = TRUE))
  )


means <- res %>%
  filter(
    earlyLifeTrt == "STD"
    , adultTrt == "LBN"
  ) %>%
  pull(bootstrapMean)

means[1]

pscProps %>%
  group_by(earlyLifeTrt, adultTrt) %>%
  bootstrapGroup(
    amplitude
    , replace = TRUE
  )
  
  
map_dbl(1:20, ~ bootstrapGroup(pscProps %>% group_by(earlyLifeTrt, adultTrt), amplitude, replace = TRUE))
list(map_dbl(1:20, ~ bootstrapGroup(pscProps %>% group_by(earlyLifeTrt, adultTrt), amplitude, replace = TRUE)))
```

```{r}
pscProps %>%
  group_by(
    earlyLifeTrt, adultTrt)
  )
```

This one seems to work.

```{r}
bootstrapGroup <- function(grouped_data, columnName, nBootstrap = 2000, replace = TRUE, setSeed = NULL) {
  if (!is.null(setSeed)) {
    set.seed(setSeed)
  }
  
  # Create one bootstrap sample and calculate its mean
  oneBootstrapSample <- function(df) {
    sample_data <- sample_n(df, size = n(), replace = replace)
    mean(sample_data[[columnName]], na.rm = TRUE)
  }
  
  # Perform bootstrapping for each group
  bootstrapMeans <- grouped_data %>%
    summarise(BootstrapMean = mean(map_dbl(1:nBootstrap, ~oneBootstrapSample(pick(everything())))),
              .groups = 'drop')

  return(bootstrapMeans)
}

# Example usage: Provide the grouped dataframe and the variable you want to bootstrap
myData <- data.frame(
  groupVar1 = rep(1:2, each = 50),
  groupVar2 = rep(1:2, times = 50),
  myVariable = rnorm(100)
)
groupedData <- myData %>% group_by(groupVar1, groupVar2)

# Now call the bootstrapGroup function with the grouped data and variable
resultingMeans <- bootstrapGroup(groupedData, 'myVariable', nBootstrap = 20)
```

```{r}
# Assuming your data frame is called pscProps with columns earlyLifeTrt, adultTrt, and amplitude.
groupedData <- pscProps %>% group_by(earlyLifeTrt, adultTrt)

# Call the bootstrapGroup function with the grouped data and variable
bootstrapResults <- bootstrapGroup(groupedData, 'amplitude', nBootstrap = 20)
```


Edit # 1 - switch to unquoted

```{r}
bootstrapGroup <- function(groupedData, columnName, nBootstrap = 2000, replace = TRUE, setSeed = NULL) {
  if (!is.null(setSeed)) {
    set.seed(setSeed)
  }
  
  # Create one bootstrap sample and calculate its mean
  oneBootstrapSample <- function(df) {
    sampleData <- sample_n(df, size = n(), replace = replace)
    mean(sampleData %>%
           select(
             {{ columnName }}
           ) %>%
           pull()
    , na.rm = TRUE)
  }
  
  # Perform bootstrapping for each group
  bootstrapMeans <- groupedData %>%
    summarise(BootstrapMean = mean(map_dbl(1:nBootstrap, ~oneBootstrapSample(pick(everything())))),
              .groups = 'drop')

  return(bootstrapMeans)
}
```

```{r}
pscProps %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  bootstrapGroup(
    amplitude
    , 20
  )
```

Edit # 2 - save the individual bootstrap results

```{r}
bootstrapGroup <- function(groupedData, columnName, nBootstrap = 2000, replace = TRUE, setSeed = NULL) {
  if (!is.null(setSeed)) {
    set.seed(setSeed)
  }
  
  # Create one bootstrap sample and calculate its mean
  oneBootstrapSample <- function(df) {
    sampleData <- sample_n(df, size = n(), replace = replace)
    mean(sampleData %>%
           select(
             {{ columnName }}
           ) %>%
           pull()
    , na.rm = TRUE)
  }
  
  # Perform bootstrapping for each group
  bootstrapMeans <- groupedData %>%
    reframe(BootstrapMean = map_dbl(1:nBootstrap, ~oneBootstrapSample(pick(everything()))))

  return(bootstrapMeans)
}
```

```{r}
pscProps %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  bootstrapGroup(
    amplitude
    , 20
  )
```

Edit # 3 - Make this specific to psc props and save each feature

This doesn't work, and we're running into the issue that it would be resampling independently for each column calculation
```{r}
bootstrapGroup <- function(df, groupingVars, nBootstrap = 2000, replace = TRUE, setSeed = NULL) {
  if (!is.null(setSeed)) {
    set.seed(setSeed)
  }
  
  # Create one bootstrap sample and calculate its mean
  oneBootstrapSample <- function(df, columnName) {
    sampleData <- sample_n(df, size = n(), replace = replace)
    mean(sampleData %>%
           select(
             {{ columnName }}
           ) %>%
           pull()
    , na.rm = TRUE)
  }
  
  # Perform bootstrapping for each group
  bootstrapMeans <- df %>%
    group_by(
      !!! groupingVars
    ) %>%
    reframe(
      amplitude = map_dbl(1:nBootstrap, ~oneBootstrapSample(pick(everything()), amplitude))
      ,riseTime = map_dbl(1:nBootstrap, ~oneBootstrapSample(pick(everything()), riseTime))
      ,decay9010 = map_dbl(1:nBootstrap, ~oneBootstrapSample(pick(everything()), decay9010))
      ,fwhm = map_dbl(1:nBootstrap, ~oneBootstrapSample(pick(everything()), fwhm))
    )
  
  # averagedBootstrapMeans <- bootstrapMeans %>%
  #   group_by(
  #     !!! groupingVars
  #   ) %>%
  #   meanSummary(
  #     c(amplitude, riseTime, decay9010, fwhm)
  #   ) %>%
  #   arrange(
  #     variable
  #   )
  # 
  # amplitudePlot <- bootstrapMeans %>%
  #   scatterPlotComboTrt(
  #     yVar = amplitude
  #     , yLab = "amplitude (pA)"
  #   )
  #   

  return(
    list(
      "bootstrapRes" = bootstrapMeans
      # , "bootstrapSummary" = averagedBootstrapMeans
      # , "amplitudePlot" = amplitudePlot
    )
  )
}
```

```{r}
pscProps %>%
  bootstrapGroup(
    exprs(earlyLifeTrt, adultTrt)
    , 20
  )
```


Edit #4 - Had issues with grouping maintained after sample_n so summarized results weren't grouped, but now using group_map without a grouped variable  

```{r}
bootstrapGroup <- function(df, nBootstrap = 2000, replace = TRUE, setSeed = NULL) {
  if (!is.null(setSeed)) {
    set.seed(setSeed)
  }
  
  # Perform bootstrapping for each group and store results in new columns
  bootstrapMeans <- bind_rows(lapply(1:nBootstrap, function(i) {
    sampleData <- group_map(df, function(.x, .y) {
      df <- sample_n(.x %>% group_by(earlyLifeTrt, adultTrt), size = n(), replace = replace)
      df %>%
        group_by(earlyLifeTrt, adultTrt) %>%
        summarize(
          amplitude = mean(amplitude, na.rm = TRUE)
          , riseTime = mean(riseTime, na.rm = TRUE)
          , decay9010 = mean(decay9010, na.rm = TRUE)
          , fwhm = mean(fwhm, na.rm = TRUE)
          , .groups = "drop"
        )
    }, .keep = TRUE)
  }))
  return(bootstrapMeans)
}
```


```{r}
pscProps %>%
  # group_by(
  #   earlyLifeTrt, adultTrt
  # ) %>%
  bootstrapGroup(
    nBootstrap = 20
  )
```

Edit #5 - Fix grouping

```{r}
bootstrapGroup <- function(groupedDF, nBootstrap = 2000, replace = TRUE, setSeed = NULL) {
  if (!is.null(setSeed)) {
    set.seed(setSeed)
  }
  
  # Perform bootstrapping for each group and store results in new columns
  bootstrapMeans <- bind_rows(lapply(1:nBootstrap, function(i) {
    sampleData <- group_map(groupedDF, function(.x, .y) {
      df <- slice_sample(.x, prop = 1, replace = replace
                         # , .preserve = TRUE
                         )
      df %>%
        # group_by(earlyLifeTrt, adultTrt) %>%
        summarize(
          amplitude = mean(amplitude, na.rm = TRUE)
          , riseTime = mean(riseTime, na.rm = TRUE)
          , decay9010 = mean(decay9010, na.rm = TRUE)
          , fwhm = mean(fwhm, na.rm = TRUE)
          , .groups = "drop"
        )
    }, .keep = TRUE)
  }))
  return(bootstrapMeans)
}
```




```{r}
list_rbind(
  map(
    pscProps %>%
      group_by(
        earlyLifeTrt
        , adultTrt
      )
    , function(.x){
      df <- slice_sample(
        .x
        , prop = 1
        , replace = TRUE
      )
      df %>%
        summarize(
          amplitude = mean(amplitude, na.rm = TRUE)
          , riseTime = mean(riseTime, na.rm = TRUE)
          , decay9010 = mean(decay9010, na.rm = TRUE)
          , fwhm = mean(fwhm, na.rm = TRUE)
          , .groups = "drop"
        )
    }
  )
)
```


```{r}
pscProps %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  slice_sample(
    prop = 1
    , replace = TRUE
  )
```



```{r}
pscProps %>%
  # group_by(
  #   earlyLifeTrt, adultTrt
  # ) %>%
  bootstrapGroup(
    nBootstrap = 20
  )
```

```{r}
mtcars %>%
  group_by(cyl) %>%
  group_map(~ head(.x, 2L), .keep = TRUE)

mtcars %>%
  group_by(cyl) %>%
  group_map(~ head(.x, 2L), .keep = FALSE)

```

## Working list of means from bootstrapping

Edit #6 - start over:


```{r}
getOneBootstrapMeans <- function(
    df
    , replace = TRUE
    , bootstrapIt = NA
){
  df <- slice_sample(
    df
    , prop = 1
    , replace = replace
  )
  sumDF <- df %>%
    summarize(
      amplitude = mean(amplitude, na.rm = TRUE)
      , riseTime = mean(riseTime, na.rm = TRUE)
      , decay9010 = mean(decay9010, na.rm = TRUE)
      , fwhm = mean(fwhm, na.rm = TRUE)
      , .groups = "drop"
    )
  if(!is.na(bootstrapIt)){
    sumDF <- sumDF %>%
      mutate(
        bootIt = bootstrapIt
      )
  }
  return(sumDF)
}

getOneBootstrapMedians <- function(
    df
    , replace = TRUE
    , bootstrapIt = NA
){
  df <- slice_sample(
    df
    , prop = 1
    , replace = replace
  )
  sumDF <- df %>%
    summarize(
      amplitude = median(amplitude, na.rm = TRUE)
      , riseTime = median(riseTime, na.rm = TRUE)
      , decay9010 = median(decay9010, na.rm = TRUE)
      , fwhm = median(fwhm, na.rm = TRUE)
      , .groups = "drop"
    )
  if(!is.na(bootstrapIt)){
    sumDF <- sumDF %>%
      mutate(
        bootIt = bootstrapIt
      )
  }
  return(sumDF)
}
```

```{r}
bind_rows(
  map(
  1: 20
  , ~ getOneBootstrapMeans(pscProps %>% group_by(earlyLifeTrt, adultTrt))
)
)
```

```{r}
bootstrapGroup <- function(groupedDF, nBootstrap = 2000, replace = TRUE, setSeed = NULL, meanOrMedian = "mean") {
  if (!is.null(setSeed)) {
    set.seed(setSeed)
  }
  
  if(meanOrMedian == "median"){
    bootstrapRes <- bind_rows(
      map(
        1:nBootstrap
        , ~ getOneBootstrapMedians(groupedDF, replace = replace, bootstrapIt = .x)
      )
    )
  } else {
    bootstrapRes <- bind_rows(
      map(
        1:nBootstrap
        , ~ getOneBootstrapMeans(groupedDF, replace = replace, bootstrapIt = .x)
      )
    )
  }
  
  
  return(bootstrapRes)
}
```

```{r}
pscProps %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  bootstrapGroup(
    nBootstrap = 20
  )
```

```{r}
analyzeBootstrapResults <- function(df, nBootstrap = 2000, replace = TRUE, setSeed = NULL, meanOrMedian = "mean", dotSize = 1, textSize = 11){
  bootstrapRes <- df %>% 
    group_by(earlyLifeTrt, adultTrt) %>% 
    bootstrapGroup(nBootstrap = nBootstrap, replace = TRUE, setSeed = setSeed, meanOrMedian = meanOrMedian)
  
  bootstrapSum <- bootstrapRes %>%
    group_by(
      earlyLifeTrt, adultTrt
    ) %>%
    meanSummary(
      col = c(amplitude, riseTime, decay9010, fwhm)
    ) %>%
    arrange(
      variable
      , adultTrt
      , earlyLifeTrt
    )
  
  # ANOVA tests
  amplitude_bootstrapAnova <- anova_test(
    bootstrapRes
    , amplitude ~ earlyLifeTrt * adultTrt
    , type = 3
  )
  riseTime_bootstrapAnova <- anova_test(
    bootstrapRes
    , riseTime ~ earlyLifeTrt * adultTrt
    , type = 3
  )
  decay9010_bootstrapAnova <- anova_test(
    bootstrapRes
    , decay9010 ~ earlyLifeTrt * adultTrt
    , type = 3
  )
  fwhm_bootstrapAnova <- anova_test(
    bootstrapRes
    , fwhm ~ earlyLifeTrt * adultTrt
    , type = 3
  )
  
  # Plots
  amplitude_plot <- bootstrapRes %>%
    combineStress() %>%
    scatterPlotComboTrt(
      yVar = amplitude
      , yLab = "amplitude (pA)"
      , dotSize = dotSize
      , fontSize = textSize
    )
  riseTime_plot <- bootstrapRes %>%
    combineStress() %>%
    scatterPlotComboTrt(
      yVar = riseTime
      , yLab = "rise time (ms)"
      , dotSize = dotSize
      , fontSize = textSize
    )
  decay9010_plot <- bootstrapRes %>%
    combineStress() %>%
    scatterPlotComboTrt(
      yVar = decay9010
      , yLab = "decay time (ms)"
      , dotSize = dotSize
      , fontSize = textSize
    )
  fwhm_plot <- bootstrapRes %>%
    combineStress() %>%
    scatterPlotComboTrt(
      yVar = fwhm
      , yLab = "full width half maximum (ms)"
      , dotSize = dotSize
      , fontSize = textSize
    )
  
  return(
    list(
      "results" = bootstrapRes
      , "summary" = bootstrapSum
      , "anovas" = list(
        "amplitude" = amplitude_bootstrapAnova
        , "riseTime" = riseTime_bootstrapAnova
        , "decayTime" = decay9010_bootstrapAnova
        , "fwhm" = fwhm_bootstrapAnova
      )
      , "plots" = list(
        "amplitude" = amplitude_plot
        , "riseTime" = riseTime_plot
        , "decayTime" = decay9010_plot
        , "fwhm" = fwhm_plot
      )
    )
  )
}
```

```{r}
pscProps %>%
  analyzeBootstrapResults(nBootstrap = 10)
```

```{r}
pscProps %>%
  analyzeBootstrapResults(nBootstrap = 10, meanOrMedian = "median")
```
```{r}
pscProps %>%
  analyzeBootstrapResults(nBootstrap = 20)
```

```{r}
pscProps_100perCell_v1 %>%
  analyzeBootstrapResults(nBootstrap = 10)
```

```{r}
pscProps_100perCell_v1 %>%
  analyzeBootstrapResults(nBootstrap = 1000)
```

This doesn't seem to work for the median. They are not normally distributed like the means are
```{r}
pscProps_100perCell_v1 %>%
  analyzeBootstrapResults(nBootstrap = 100, meanOrMedian = "median")
```

```{r}
max100PerCell_v1Res$amplitude$plots$cumulativeFreqPlot
```


```{r}
figGABA2a_model
allEventsRes$amplitude$plots$cumulativeFreqPlot
```


```{r}
bootstrapRes_1000_100perCell_v1 <- pscProps_100perCell_v1 %>%
  analyzeBootstrapResults(
    nBootstrap = 1000
    , setSeed = 928
  )

bootstrapRes_1000_100perCell_v2 <- pscProps_100perCell_v2 %>%
  analyzeBootstrapResults(
    nBootstrap = 1000
    , setSeed = 928
  )

bootstrapRes_1000_100perCell_v3 <- pscProps_100perCell_v3 %>%
  analyzeBootstrapResults(
    nBootstrap = 1000
    , setSeed = 928
  )
```

# Num events by prop scatters

## Amplitude

```{r}
pscProps %>%
  left_join(
    pscProps %>%
    group_by(cellID) %>%
    summarize(
      numEvents = n()
      , .groups = "drop"
    )
    , by = "cellID"
  ) %>%
  combineStress() %>%
  scatterPlotTwoVars_byComboTrt(
    yVar = amplitude
    , yLab = "amplitude (pA)"
    , xVar = numEvents
    , xLab = "events per cell"
    , zoom_y = TRUE
    , ymin = 0
    , ymax = 125
  ) +
  geom_smooth(
    aes(color = comboTrt)
    , method = "lm"
    # , method = "loess"
    # , method = "gam"
  )
```
```{r}
pscProps %>%
  left_join(
    pscProps %>%
    group_by(cellID) %>%
    summarize(
      numEvents = n()
      , .groups = "drop"
    )
    , by = "cellID"
  ) %>%
  combineStress() %>%
  scatterPlotTwoVars_byComboTrt(
    yVar = riseTime
    , yLab = "rise time (ms)"
    , xVar = numEvents
    , xLab = "events per cell"
    , zoom_y = TRUE
    , ymin = 0
    , ymax = 2.2
  ) +
  geom_smooth(
    aes(color = comboTrt)
    , method = "lm"
    # , method = "loess"
    # , method = "gam"
  )
```
```{r}
pscProps %>%
  left_join(
    pscProps %>%
    group_by(cellID) %>%
    summarize(
      numEvents = n()
      , .groups = "drop"
    )
    , by = "cellID"
  ) %>%
  combineStress() %>%
  scatterPlotTwoVars_byComboTrt(
    yVar = decay9010
    , yLab = "decay time (ms)"
    , xVar = numEvents
    , xLab = "events per cell"
    , zoom_y = TRUE
    , ymin = 0
    , ymax = 80
  ) +
  geom_smooth(
    aes(color = comboTrt)
    , method = "lm"
    # , method = "loess"
    # , method = "gam"
  )
```
```{r}
pscProps %>%
  left_join(
    pscProps %>%
    group_by(cellID) %>%
    summarize(
      numEvents = n()
      , .groups = "drop"
    )
    , by = "cellID"
  ) %>%
  combineStress() %>%
  scatterPlotTwoVars_byComboTrt(
    yVar = fwhm
    , yLab = "fwhm (ms)"
    , xVar = numEvents
    , xLab = "events per cell"
    , zoom_y = TRUE
    , ymin = 0
    , ymax = 20
  ) +
  geom_smooth(
    aes(color = comboTrt)
    , method = "lm"
    # , method = "loess"
    # , method = "gam"
  )
```

# interval

```{r}
allEventsRes_int <- pscInt %>%
  filter(
    !is.na(interval)
  ) %>%
  fourWayAD(
    interval
    , "interval (s)"
    , zoom_x = TRUE
    , xmin = 0
    , xmax = 20
  )

allEventsRes_int$plots$cumulativeFreqPlot
```

```{r}
pscInt %>%
  left_join(
    pscInt %>%
    group_by(cellID) %>%
    summarize(
      numEvents = n()
      , .groups = "drop"
    )
    , by = "cellID"
  ) %>%
  filter(
    !is.na(interval)
  ) %>%
  scatterPlotTwoVars_byComboTrt(
    yVar = interval
    , yLab = "interval (s)"
    , xVar = numEvents
    , xLab = "events per cell"
    , zoom_y = TRUE
    , ymin = 0
    , ymax = 20
  ) +
  geom_smooth(
    aes(color = comboTrt)
    , method = "lm"
    # , method = "loess"
    # , method = "gam"
  )
```
```{r}
pscInt_numEventsPerCell <- pscInt %>%
    group_by(cellID, earlyLifeTrt, adultTrt) %>%
    summarize(
      numEvents = n()
      , .groups = "drop"
    )

pscInt_numEventsPerCell %>%
  plotCumulativeFreqDist (
    numEvents
    , "events per cell"
)

pscInt_numEventsPerCell %>%
  group_by(earlyLifeTrt, adultTrt) %>%
  summarize(
    nCellsLess300 = sum(numEvents < 300)
    , nCells = n()
  )

pscInt_numEventsPerCell %>%
  group_by(earlyLifeTrt, adultTrt) %>%
  quartilesSummary(
    numEvents
  ) %>%
  relocate(
    max, .after = q3
  )
```

Preparing comparison of bootstrap differences

```{r}
# need to calculate observed difference in original dataset before sampling

orig_diff <- pscProps %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  summarize(
    amplitude = mean(amplitude, na.rm = TRUE)
    , riseTime = mean(riseTime, na.rm = TRUE)
    , decay9010 = mean(decay9010, na.rm = TRUE)
    , fwhm = mean(fwhm, na.rm = TRUE)
    , .groups = "drop"
  ) %>%
  pivot_longer(
    cols = c(amplitude, riseTime, decay9010, fwhm)
    , names_to = "variable"
    , values_to = "mean"
  ) %>%
  pivot_wider(
    id_cols = c(variable)
    , names_from = c(earlyLifeTrt, adultTrt)
    , names_sep = "_"
    , values_from = mean
  ) %>%
  mutate(
    orig_stdCONvALPS = STD_ALPS - STD_CON
    , orig_lbnCONvALPS = LBN_ALPS - LBN_CON
    , orig_conSTDvLBN = LBN_CON - STD_CON
    , orig_alpsSTDvLBN = LBN_ALPS - STD_ALPS
    , orig_STD_CONvLBN_ALPS = LBN_ALPS - STD_CON
    , orig_STD_ALPSvLBN_CON = LBN_CON - STD_ALPS
  ) %>%
  select(
    -c(STD_ALPS, STD_CON, LBN_ALPS, LBN_CON)
  )


propLargerDiff <- bootstrapRes_iterations10_maxPerCellNone$results %>%
  pivot_longer(
    cols = c(amplitude, riseTime, decay9010, fwhm)
    , names_to = "variable"
    , values_to = "mean"
  ) %>%
  pivot_wider(
    id_cols = c(bootIt, variable)
    , names_from = c(earlyLifeTrt, adultTrt)
    , names_sep = "_"
    , values_from = mean
  ) %>%
  mutate(
    stdCONvALPS = STD_ALPS - STD_CON
    , lbnCONvALPS = LBN_ALPS - LBN_CON
    , conSTDvLBN = LBN_CON - STD_CON
    , alpsSTDvLBN = LBN_ALPS - STD_ALPS
    , STD_CONvLBN_ALPS = LBN_ALPS - STD_CON
    , STD_ALPSvLBN_CON = LBN_CON - STD_ALPS
  ) %>%
  left_join(
    orig_diff
    , by = "variable"
  ) %>%
  mutate(
    stdCONvALPS_isBigger = (stdCONvALPS <= -abs(orig_stdCONvALPS) | stdCONvALPS >= abs(orig_stdCONvALPS))
    , lbnCONvALPS_isBigger = (lbnCONvALPS <= -abs(orig_lbnCONvALPS) | lbnCONvALPS >= abs(orig_lbnCONvALPS))
    , conSTDvLBN_isBigger = (conSTDvLBN <= -abs(orig_conSTDvLBN) | conSTDvLBN >= abs(orig_conSTDvLBN))
    , alpsSTDvLBN_isBigger = (alpsSTDvLBN <= -abs(orig_alpsSTDvLBN) | alpsSTDvLBN >= abs(orig_alpsSTDvLBN))
    , STD_CONvLBN_ALPS_isBigger = (STD_CONvLBN_ALPS <= -abs(orig_STD_CONvLBN_ALPS) | STD_CONvLBN_ALPS >= abs(orig_STD_CONvLBN_ALPS))
    , STD_ALPSvLBN_CON_isBigger = (STD_ALPSvLBN_CON <= -abs(orig_STD_ALPSvLBN_CON) | STD_ALPSvLBN_CON >= abs(orig_STD_ALPSvLBN_CON))
  ) %>%
  group_by(
    variable
  ) %>%
  summarize( # these are p-values, 1 - proportion where larger than original difference
    stdCONvALPS = 1 - mean(stdCONvALPS_isBigger)
    , lbnCONvALPS = 1 - mean(lbnCONvALPS_isBigger)
    , conSTDvLBN = 1 - mean(conSTDvLBN_isBigger)
    , alpsSTDvLBN = 1- mean(alpsSTDvLBN_isBigger)
    , STD_CONvLBN_ALPS = 1 - mean(STD_CONvLBN_ALPS_isBigger)
    , STD_ALPSvLBN_CON = 1- mean(STD_ALPSvLBN_CON_isBigger)
    , .groups = "drop"
  ) %>%
  pivot_longer(
    cols = stdCONvALPS:STD_ALPSvLBN_CON
    , names_to = "comparison"
    , values_to = "p.value"
  )

propLargerDiff %>%
  group_by(
    variable
  ) %>%
  adjust_pvalue(p.col = "p.value")

propLargerDiff %>%
  group_by(
    variable
  ) %>%
  group_map(~ adjust_pvalue(.x, p.col = "p.value"))
  adjust_pvalue(p.col = "p.value")
```

