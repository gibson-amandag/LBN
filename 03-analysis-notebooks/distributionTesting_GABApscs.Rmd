---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
pscProps <- loadExcelSheet(dataFolder, LBN_DataName, "pscProps")

# pscProps <- pscProps %>%
#   makeFactors(
#     cols = cellID
#   )

pscProps <- pscProps %>%
  left_join(
    GABApscs_240 %>%
      select(
        cellID
        , mouseID
        , damID
        , earlyLifeTrt
        , adultTrt
      )
    , by = "cellID"
  )

saveDFsToCSV(
  saveFolder = "./"
  ,"pscProps" = pscProps
)
```

```{r}
pscProps %>%
  group_by(
    cellID
  ) %>%
  meanSummary(
    relPeak
  )
```


```{r}
GABApscs_240
```


Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.


Quantile summary

```{r}
install.packages("lqmm")
```

```{r}

library(lqmm)

```

```{r}

# Assuming you have:
# - `data`: Your dataset.
# - `property`: A dependent variable.
# - `earlyLifeTrt`: Early life treatment factor.
# - `adultTrt`: Adult treatment factor.
# - `mouseID`: Mouse ID factor.
# - `cellID`: Cell ID factor.

# Let's create a list to store models for different quantiles
quantiles <- c(0.25, 0.5, 0.75)
models <- list()

# for (tau in quantiles) {
#  models[[as.character(tau)]] <- lqmm(
#     fixed = relPeak ~ earlyLifeTrt * adultTrt, 
#     random = ~ 1 | mouseID/cellID,
#     tau = tau, 
#     data = data
#   )
# 
# # Let's examine the summaries for each quantile
# for (tau in names(models)) {
#   cat("Summary for tau =", tau, "quantile:\n")
#   print(summary(models[[tau]]))
#   cat("\n\n")
# }

for (tau in quantiles) {
  models[[as.character(tau)]] <- lqmm(
    fixed = relPeak ~ earlyLifeTrt * adultTrt, 
    random = ~ 1 | mouseID/cellID,
    tau = tau, 
    data = pscProps
  )
}

# Let's examine the summaries for each quantile
# This loop is separate from the one above and therefore needs its own opening curly brace.
for (tau in names(models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(models[[tau]]))
  cat("\n\n")
}
```


```{r}
# Fit models for specified quantiles
for (tau in quantiles) {
  model <- lqmm(
    fixed = relPeak ~ earlyLifeTrt * adultTrt, 
    random = ~ 1 | mouseID/cellID,
    tau = tau, 
    data = pscProps
  )
  models[[as.character(tau)]] <- model
}

# Examine the summaries for each quantile
for (tau in names(models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(models[[tau]]))
  cat("\n\n")
}
```


```{r}
model <- lqmm(
  fixed = relPeak ~ earlyLifeTrt * adultTrt,
  random = ~ 1 | mouseID,
  tau = 0.50,
  data = pscProps
)

```

```{r}
model <- lqmm(
  fixed = relPeak ~ 1,  # A simple intercept-only model
  random = ~ 1 | mouseID,
  tau = 0.50,
  data = pscProps
)

summary(model)
```

```{r}
install.packages("lqmm")
```


```{r}
library(lqmm)
# set.seed(123)  # Set seed for reproducibility

# Create sample data
num_mice <- 30  # Let's assume we have 30 mice
num_measurements <- 5  # Each mouse has 5 measurements

# Generate a data frame
sample_data <- expand.grid(
  mouseID = factor(1:num_mice),       # Mouse ID factor
  measurementID = 1:num_measurements  # Measurement within each mouse
)

# Simulate a response variable with some random noise and a treatment effect
sample_data$response <- 10 +  # Some baseline response level
                         ifelse(sample_data$mouseID %% 2 == 0, 3, -2) +  # Treatment A or B effect
                         rnorm(nrow(sample_data), mean = 0, sd = 1)  # Add some noise

# Simulate a treatment factor with two levels
# (e.g., even mouse IDs receive treatment A, odd receive treatment B)
sample_data$treatment <- ifelse(sample_data$mouseID %% 2 == 0, "TreatmentA", "TreatmentB")
sample_data$treatment <- as.factor(sample_data$treatment)  # Convert to factor

# Simple lqmm with random intercept for each mouse
model <- lqmm(
  fixed = response ~ treatment,
  random = ~ 1 | mouseID,
  tau = 0.5,  # Fitting at the median (0.5 quantile)
  data = sample_data
)

# View the summary of the model
summary(model)
```

```{r}
model <- lqmm(fixed = response ~ treatment, tau = 0.5, data = sample_data)
```


```{r}
# We will also assume that pscProps$relPeak, pscProps$earlyLifeTrt, and pscProps$adultTrt are appropriately formatted.
# Convert mouseID to character as shown in the Orthodont example, if it's not already:
pscProps$mouseID <- as.character(pscProps$mouseID)
pscProps$cellID <- as.character(pscProps$cellID)

# Fit a simple quantile regression with random intercept for mouseID
# Note: 'group' instead of 'groups', no nesting with '/', and mouseID as a character
fit_lqmm <- lqmm(
  fixed = relPeak ~ earlyLifeTrt * adultTrt,
  random = ~ 1,
  group = mouseID,
  tau = 0.5,  # median
  data = pscProps
)

# Check the summary
summary(fit_lqmm)
```

```{r}
# Generate sample data
sample_size <- 100  # Define the total number of observations

# Create a data frame with a binary treatment variable and a random mouseID
sample_data <- data.frame(
  mouseID = as.character(rep(1:10, each = sample_size / 10)), # 10 mice, each with 10 measurements
  treatment = sample(rep(c("Control", "Treatment"), each = sample_size / 2)), # Binary treatment
  response = rnorm(sample_size, mean = 100, sd = 10) # Response with normal distribution
)

# Add an effect of treatment and random noise by mouseID
sample_data$response[sample_data$treatment == "Treatment"] <- sample_data$response[sample_data$treatment == "Treatment"] + 20
sample_data$response <- sample_data$response + rnorm(n = 10, mean = 0, sd = 5)[as.numeric(sample_data$mouseID)]

# Convert 'treatment' to a factor
sample_data$treatment <- as.factor(sample_data$treatment)

# Fit a quantile regression model using lqmm
fit_sample_lqmm <- lqmm(
  fixed = response ~ treatment,
  random = ~ 1,
  group = mouseID,
  tau = 0.5,  # median
  data = sample_data
)

# Check the summary of the model
summary(fit_sample_lqmm)
```


```{r}
str(pscProps)
```

```{r}
summary(pscProps$relPeak)
```


```{r}
levels(pscProps$earlyLifeTrt)
levels(pscProps$adultTrt)
```

```{r}
sum(is.na(pscProps$relPeak))
sum(is.nan(pscProps$relPeak))
sum(is.infinite(pscProps$relPeak))

any(is.na(pscProps$mouseID))
any(is.na(pscProps$earlyLifeTrt))
any(is.na(pscProps$adultTrt))
```

```{r}
table(pscProps$mouseID)
```

```{r}
head(pscProps, n = 20)  # Show the first 20 rows of the data
```


```{r}
# Convert 'mouseID' to a factor
pscProps$mouseID <- as.factor(pscProps$mouseID)

# Fit a basic quantile regression model with random intercepts for 'mouseID'
fit_basic_lqmm <- lqmm(
  fixed = relPeak ~ earlyLifeTrt + adultTrt,
  random = ~ 1,
  group = mouseID,
  tau = 0.25,  # median
  data = pscProps
)

# Check the summary of the model
summary(fit_basic_lqmm)
```


```{r}
subset_pscProps <- pscProps[pscProps$mouseID %in% c("701", "702", "713", "714"),]

# Fit the model on the subset
fit_basic_lqmm_subset <- lqmm(
  fixed = relPeak ~ earlyLifeTrt + adultTrt,
  random = ~ 1,
  group = mouseID,
  tau = 0.5,
  data = subset_pscProps
)

pscProps %>%
  group_by(
    mouseID
    , earlyLifeTrt
    , adultTrt
  ) %>%
  summarize(
    n()
  )
```

This one works with fwhm. It doesn't for relPeak
```{r}
testModel <- lqmm(fwhm ~ earlyLifeTrt * adultTrt
     , random = ~1
     , group = cellID
     , tau = 0.5
     , data = pscProps
   )
```
This one works with fwhm. It doesn't for relPeak
```{r}
testModel <- lqmm(fwhm ~ earlyLifeTrt * adultTrt
     , random = ~ 1
     , group = cellID
     , tau = 0.5
     , data = pscProps
   )
```

# Full width half max - Quantile analysis - works

```{r}
# Let's create a list to store models for different quantiles
quantiles <- c(
  0.25
  , 0.5
  , 0.75
)
models <- list()

for (tau in quantiles) {
  models[[as.character(tau)]] <- lqmm(
    fwhm ~ earlyLifeTrt * adultTrt
    , random = ~ 1
    , group = cellID
    , tau = tau
    , data = pscProps
    , control = lqmmControl(
      LP_max_iter = 2000
    ) 
   )
  
}

# Let's examine the summaries for each quantile
for (tau in names(models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(models[[tau]]))
  cat("\n\n")
}
```

```{r}
fwhm_quartile <- pscProps %>%
  group_by(
    cellID
    , mouseID
    , damID
    , earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    fwhm
  )

anova_test(
  fwhm_quartile
  , q1 ~ earlyLifeTrt * adultTrt
  , type = 3
)

mixed(
  q1 ~ earlyLifeTrt * adultTrt + (1|mouseID) + (1|damID)
  , fwhm_quartile
  , method = "KR"
)
```
```{r}
relPeak_quartile <- pscProps %>%
  group_by(
    cellID
    , mouseID
    , damID
    , earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    relPeak
  )

anova_test(
  relPeak_quartile
  , q1 ~ earlyLifeTrt * adultTrt
  , type = 3
)

mixed(
  q1 ~ earlyLifeTrt * adultTrt + (1|mouseID) + (1|damID)
  , relPeak_quartile
  , method = "KR"
)
anova_test(
  relPeak_quartile
  , median ~ earlyLifeTrt * adultTrt
  , type = 3
)

mixed(
  median ~ earlyLifeTrt * adultTrt + (1|mouseID) + (1|damID)
  , relPeak_quartile
  , method = "KR"
)
anova_test(
  relPeak_quartile
  , q3 ~ earlyLifeTrt * adultTrt
  , type = 3
)

mixed(
  q3 ~ earlyLifeTrt * adultTrt + (1|mouseID) + (1|damID)
  , relPeak_quartile
  , method = "KR"
)
```


# Rise time - Quantile analysis - works

```{r}
# Let's create a list to store models for different quantiles
quantiles <- c(
  0.25
  , 0.5
  , 0.75
)
models <- list()

for (tau in quantiles) {
  models[[as.character(tau)]] <- lqmm(
    riseTime ~ earlyLifeTrt * adultTrt
    , random = ~ 1
    , group = cellID
    , tau = tau
    , data = pscProps
    , control = lqmmControl(
      LP_max_iter = 2000
    ) 
   )
  
}

# Let's examine the summaries for each quantile
for (tau in names(models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(models[[tau]]))
  cat("\n\n")
}
```

# Decay time - Quantile analysis - doesn't work

```{r}
# Let's create a list to store models for different quantiles
quantiles <- c(
  0.25
  , 0.5
  , 0.75
)
models <- list()

for (tau in quantiles) {
  models[[as.character(tau)]] <- lqmm(
    decay9010 ~ earlyLifeTrt * adultTrt
    , random = ~ 1
    , group = cellID
    , tau = tau
    , data = pscProps
    , control = lqmmControl(
      LP_max_iter = 2000
    )
   )
  
}

# Let's examine the summaries for each quantile
for (tau in names(models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(models[[tau]]))
  cat("\n\n")
}
```
# Relative peak - quantile analysis - doesn't work

```{r}
# Let's create a list to store models for different quantiles
quantiles <- c(
  0.25
  , 0.5
  , 0.75
)
models <- list()

for (tau in quantiles) {
  models[[as.character(tau)]] <- lqmm(
    relPeak ~ earlyLifeTrt * adultTrt
    , random = ~ 1
    , group = cellID
    , tau = tau
    , data = pscProps
    , control = lqmmControl(
      LP_max_iter = 2000
    ) 
   )
  
}

# Let's examine the summaries for each quantile
for (tau in names(models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(models[[tau]]))
  cat("\n\n")
}
```

Remove the lowest 1% and highest 1% of data
```{r}
lower_bound <- quantile(pscProps$relPeak, probs = 0.2)
upper_bound <- quantile(pscProps$relPeak, probs = 0.8)

pscProps_outliersRemoved <- pscProps %>%
  filter(
    relPeak > lower_bound & relPeak < upper_bound
  )

relPeak_models <- list()

for (tau in quantiles) {
  relPeak_models[[as.character(tau)]] <- lqmm(
    relPeak ~ earlyLifeTrt * adultTrt
    , random = ~ 1
    , group = cellID
    , tau = tau
    , data = pscProps_outliersRemoved
    , control = lqmmControl(
      LP_max_iter = 2000
    ) 
  )
  
}

# Summaries for each quantile
for (tau in names(relPeak_models)) {
  cat("Summary for tau =", tau, "quantile:\n")
  print(summary(relPeak_models[[tau]]))
  cat("\n\n")
}
```

```{r}
summary(lqmm(
  fixed = fwhm ~ earlyLifeTrt * adultTrt
  , random = ~ 1
  , group = cellID
  , tau = c(.25, .5, .75)
  , data = pscProps
  , control = lqmmControl(
    LP_max_iter = 2000
  )
))
```


```{r}
# Base R plot
qqnorm(pscProps$relPeak)
qqline(pscProps$relPeak) # Adds a trend line

# ggplot2
library(ggplot2)

ggplot(pscProps, aes(sample = relPeak)) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of relPeak") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles") +
  theme_minimal()
```



# kSamples distributions

```{r}
install.packages("kSamples")
```

```{r}
library(kSamples)
```

## 4-way Anderson-Darling

```{r}
group_STD_CON <- pscProps$relPeak[pscProps$earlyLifeTrt == "STD" & pscProps$adultTrt == "CON"]
group_STD_ALPS <- pscProps$relPeak[pscProps$earlyLifeTrt == "STD" & pscProps$adultTrt == "ALPS"]
group_LBN_CON <- pscProps$relPeak[pscProps$earlyLifeTrt == "LBN" & pscProps$adultTrt == "CON"]
group_LBN_ALPS <- pscProps$relPeak[pscProps$earlyLifeTrt == "LBN" & pscProps$adultTrt == "ALPS"]

# Perform the Anderson-Darling test to compare the four distributions
ad_test_result <- ad.test(group_STD_CON, group_STD_ALPS, group_LBN_CON, group_LBN_ALPS)

# Output the results
print(ad_test_result)
```

### Pairwise

```{r}
groups_list <- list(
  "STD-CON" = group_STD_CON
  , "STD-ALPS" = group_STD_ALPS
  , "LBN-CON" = group_LBN_CON
  , "LBN-ALPS" = group_LBN_ALPS
)
# Pairwise Anderson-Darling tests
pairs <- combn(length(groups_list), 2, simplify = FALSE)
pairwise_results <- lapply(pairs, function(p) {
  list(groups = p, 
       test_result = ad.test(groups_list[[p[1]]], groups_list[[p[2]]]))
})

# Print the pairwise test results
for (result in pairwise_results) {
  cat("Comparing Group", result$groups[1], "and Group", result$groups[2], ":\n")
  print(result$test_result)
  cat("\n")
}
```

```{r}
getAD_Pval <- function(result, version = 1, fromPaired = FALSE){
  if(fromPaired){
    result <- result$test_result
  }
  tbl <- result$ad
  tbl_tibble <- tbl %>% as.tibble()
  p_val <- tbl_tibble$` asympt. P-value`[[version]]
  return(p_val)
}
```


### Adjusted
```{r}
# Assuming 'pairwise_results' contains the results of your pairwise Anderson-Darling tests
# Extracting p-values from the test results
p_values <- sapply(pairwise_results, function(result){
  p_val <- getAD_Pval(result, fromPaired = TRUE)
  return(p_val)
})

# Apply the Holm correction to the p-values
p_adjusted <- p.adjust(p_values, method = "holm")

# Pairwise comparison labels
pair_labels <- sapply(pairs, function(p) paste("Group", p[1], "vs Group", p[2]))

# Combine the labels and adjusted p-values into a data frame for a clearer presentation
comparison_results <- data.frame(
  Pair = pair_labels,
  P_Value = p_values,
  P_Adjusted = p_adjusted
)

# Print the results with Holm correction
print(comparison_results)
```


Missing the pairwise


```{r}
# Bootstrap undersampling function
undersample_data <- function(data, max_per_cell = 15) {
  # Split data by cellID
  data_split <- split(data, data$cellID)
  
  # Sample up to max_per_cell observations per cellID
  data_sampled <- do.call("rbind", lapply(data_split, function(events) {
    if(nrow(events) > max_per_cell) {
      events[sample(1:nrow(events), max_per_cell), ]
    } else {
      events
    }
  }))
  
  return(data_sampled)
}

# Initialize list to store ad.test results from each bootstrap iteration
bootstrap_results <- vector("list", 500)

# Perform the ad.test 200 times using undersampled data
for (i in 1:500) {
  # Create an undersampled dataset
  undersampled_data <- undersample_data(pscProps)
  
  # Calculate the Anderson-Darling test from the undersampled data
  group_STD_CON <- undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "STD" & undersampled_data$adultTrt == "CON"]
  group_STD_ALPS <- undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "STD" & undersampled_data$adultTrt == "ALPS"]
  group_LBN_CON <- undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "LBN" & undersampled_data$adultTrt == "CON"]
  group_LBN_ALPS <- undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "LBN" & undersampled_data$adultTrt == "ALPS"]
  
  # Perform the 4-way Anderson-Darling test
  ad_test_result <- kSamples::ad.test(group_STD_CON, group_STD_ALPS, group_LBN_CON, group_LBN_ALPS)
  
  # Store results in list
  bootstrap_results[[i]] <- ad_test_result
}

# Extract p-values from each result and apply the Holm correction
bootstrap_p_values <- sapply(bootstrap_results, function(result){
  p_val <- getAD_Pval(result)
  return(p_val)
})

average_p_value <- format(round(mean(bootstrap_p_values), 3), nsmall = 3)

print(paste("Average p-value across bootstrap replications:", average_p_value))
```

```{r}
pscProps %>%
  group_by(
    cellID
    , earlyLifeTrt
    , adultTrt
  ) %>%
  summarize(
    nEvents = n()
    , .groups = "drop"
  ) %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    nEvents
  )
```


Saving the mean vectors - doesn't work

```{r}
# Extended bootstrap undersampling function to return list of vectors
undersample_data_groups <- function(data, max_per_cell = 15) {
  # Split data by cellID
  data_split <- split(data, data$cellID)
  
  # Sample up to max_per_cell observations per cellID
  data_sampled <- do.call("rbind", lapply(data_split, function(events) {
    if(nrow(events) > max_per_cell) {
      events[sample(1:nrow(events), max_per_cell), ]
    } else {
      events
    }
  }))

  # Split into groups after undersampling
  group_vectors <- list(
    STD_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "CON"],
    STD_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "ALPS"],
    LBN_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "CON"],
    LBN_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "ALPS"]
  )
  
  return(group_vectors)
}

# Create matrices to store accumulated values for each group
group_STD_CON_accum <- matrix(nrow=500, ncol=15)
group_STD_ALPS_accum <- matrix(nrow=500, ncol=15)
group_LBN_CON_accum <- matrix(nrow=500, ncol=15)
group_LBN_ALPS_accum <- matrix(nrow=500, ncol=15)

# Perform the ad.test 500 times using undersampled data
for (i in 1:500) {
  # Get the undersampled group vectors
  groups <- undersample_data_groups(pscProps)
  
  # Accumulate the values
  group_STD_CON_accum[i, ] <- groups$STD_CON
  group_STD_ALPS_accum[i, ] <- groups$STD_ALPS
  group_LBN_CON_accum[i, ] <- groups$LBN_CON
  group_LBN_ALPS_accum[i, ] <- groups$LBN_ALPS
}

# Function to calculate the mean across each column (observation) in the matrices
mean_across_iterations <- function(matrix) {
  colMeans(matrix, na.rm=TRUE)
}

# Calculate the average vectors across all bootstrap iterations
average_vector_STD_CON <- mean_across_iterations(group_STD_CON_accum)
average_vector_STD_ALPS <- mean_across_iterations(group_STD_ALPS_accum)
average_vector_LBN_CON <- mean_across_iterations(group_LBN_CON_accum)
average_vector_LBN_ALPS <- mean_across_iterations(group_LBN_ALPS_accum)

# Perform a final Anderson-Darling test using the average vectors
final_ad_test_result <- kSamples::ad.test(
  average_vector_STD_CON,
  average_vector_STD_ALPS,
  average_vector_LBN_CON,
  average_vector_LBN_ALPS
)

# Output the final test result
print(final_ad_test_result)
```

This only gives one value for each group
```{r}
# Extended bootstrap undersampling function to return list of vectors
undersample_data_groups <- function(data, max_per_cell = 15) {
  # Split data by cellID
  data_split <- split(data, data$cellID)
  
  # Sample up to max_per_cell observations per cellID
  data_sampled <- do.call("rbind", lapply(data_split, function(events) {
    if(nrow(events) > max_per_cell) {
      events[sample(1:nrow(events), max_per_cell), ]
    } else {
      events
    }
  }))

  # Split into groups after undersampling
  group_vectors <- list(
    STD_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "CON"],
    STD_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "ALPS"],
    LBN_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "CON"],
    LBN_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "ALPS"]
  )
  
  return(group_vectors)
}

# Function to compile all values into a list while doing the bootstrap
accumulate_samples <- function(group_list, group_samples) {
  for (group_name in names(group_list)) {
    group_list[[group_name]] <- c(group_list[[group_name]], group_samples[[group_name]])
  }
  return(group_list)
}

# Initialize lists to accumulate values
group_samples_list <- list(
  STD_CON = c(),
  STD_ALPS = c(),
  LBN_CON = c(),
  LBN_ALPS = c()
)

# Perform the ad.test 500 times using undersampled data
for (i in 1:500) {
  # Get the undersampled group vectors
  groups <- undersample_data_groups(pscProps)
  
  # Accumulate sample values across all bootstrap iterations
  group_samples_list <- accumulate_samples(group_samples_list, groups)
}

# Function to calculate the mean of vectors in a list
mean_of_vector_list <- function(vector_list) {
  sapply(vector_list, function(x) mean(x, na.rm = TRUE))
}

# Calculate the average vector for each group
average_vectors <- mean_of_vector_list(group_samples_list)

# Conduct the Anderson-Darling test
final_ad_test_result <- ad.test(
  x = list(
    average_vectors[['STD_CON']],
    average_vectors[['STD_ALPS']],
    average_vectors[['LBN_CON']],
    average_vectors[['LBN_ALPS']]
  )
)

# Output the final test result
print(final_ad_test_result)
```

```{r}
# Extended bootstrap undersampling function to return list of vectors
undersample_data_groups <- function(data, max_per_cell = 15) {
  # Split data by cellID
  data_split <- split(data, data$cellID)
  
  # Sample up to max_per_cell observations per cellID
  data_sampled <- do.call("rbind", lapply(data_split, function(events) {
    if(nrow(events) > max_per_cell) {
      events[sample(1:nrow(events), max_per_cell), ]
    } else {
      events
    }
  }))

  # Split into groups after undersampling
  group_vectors <- list(
    STD_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "CON"],
    STD_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "ALPS"],
    LBN_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "CON"],
    LBN_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "ALPS"]
  )
  
  return(group_vectors)
}

# Function to accumulate samples in a list of vectors for each group
accumulate_samples <- function(accumulated_samples, new_samples) {
  lapply(names(accumulated_samples), function(group) {
    list(c(accumulated_samples[[group]], new_samples[[group]]))
  })
}

# Initialize lists to accumulate values for each group (one list per group)
accumulated_samples_list <- list(
  STD_CON = vector("list", 500),
  STD_ALPS = vector("list", 500),
  LBN_CON = vector("list", 500),
  LBN_ALPS = vector("list", 500)
)

# Perform the bootstrap 500 times using undersampled data
for (i in 1:500) {
  groups <- undersample_data_groups(pscProps) # Your existing function
  
  # Accumulate the samples across all bootstrap iterations for each group
  accumulated_samples_list <- lapply(names(accumulated_samples_list), function(group) {
    accumulated_samples_list[[group]][[i]] <- groups[[group]]
    accumulated_samples_list[[group]]
  })
}

accumulated_samples_list[["STD_CON"]]

# Now, accumulated_samples_list contains a list of vectors for the distribution of relPeak for each group

# To calculate average vectors for each group
average_vectors <- lapply(accumulated_samples_list, function(group_list) {
  mean_values <- sapply(group_list, mean, na.rm = TRUE)
  mean(mean_values, na.rm = TRUE)
})

# Now average_vectors contains an average of means for each bootstrap sample per treatment group.
# The average_vectors will look like this: list(STD_CON = X.XX, STD_ALPS = X.XX, ...)
print(average_vectors[["STD_CON"]])
```






```{r}
# Bootstrap undersampling function (same as before)
undersample_data <- function(data, max_per_cell = 20) {
  # ... (Function definition remains the same as previously provided)
}

# Function for performing all pairwise Anderson-Darling tests
pairwise_ad_test <- function(groups_list) {
  pairs <- combn(names(groups_list), 2, simplify = FALSE)
  pairwise_results <- lapply(pairs, function(p) {
    list(groups = p, 
         test_result = ad.test(groups_list[[p[1]]], groups_list[[p[2]]]))
  })
  
  # Extract p-values
  p_values <- sapply(pairwise_results, function(result) result$test_result$p.value)
  
  # Holm correction
  p_adjusted <- p.adjust(p_values, method = "holm")
  names(p_adjusted) <- sapply(pairs, function(p) paste(p[1], "vs", p[2]))
  
  return(p_adjusted)
}

# Store p-value matrices for each bootstrap iteration
p_value_matrices <- vector("list", 200)

# Perform 200 bootstrap replications
for (i in 1:200) {
  # Create an undersampled dataset
  undersampled_data <- undersample_data(pscProps)
  
  # Split into groups
  groups_list <- list(
    STD_CON = undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "STD" & undersampled_data$adultTrt == "CON"],
    STD_ALPS = undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "STD" & undersampled_data$adultTrt == "ALPS"],
    LBN_CON = undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "LBN" & undersampled_data$adultTrt == "CON"],
    LBN_ALPS = undersampled_data$relPeak[undersampled_data$earlyLifeTrt == "LBN" & undersampled_data$adultTrt == "ALPS"]
  )
  
  # Perform pairwise Anderson-Darling tests with Holm correction
  p_value_matrices[[i]] <- pairwise_ad_test(groups_list)
}

# Average the adjusted p-values across all pairwise comparisons and bootstrap replications
average_pvalues <- Reduce("+", p_value_matrices) / length(p_value_matrices)

# Display the averaged adjusted p-values
print(average_pvalues)
```



# Getting an average vector for each group and then running the anderson-darling test on this

```{r}
average_sorted_vectors <- function(vector_list, sortDec = FALSE) {
  # Check if all vectors are of the same length
  lengths <- sapply(vector_list, length)
  unique_lengths <- unique(lengths)
  
  if(length(unique_lengths) != 1) {
    stop("Not all vectors are of the same length.")
  }
  
  # Sort each vector individually
  sorted_vectors <- lapply(vector_list, function(x) sort(x, decreasing = sortDec))
  
  # Calculate the average for each position
  average_vector <- Reduce("+", sorted_vectors) / length(sorted_vectors)
  
  # Return the average vector
  return(average_vector)
}

# Extended bootstrap undersampling function to return list of vectors
undersample_data_groups <- function(data, max_per_cell = 15) {
  # Split data by cellID
  data_split <- split(data, data$cellID)
  
  # Sample up to max_per_cell observations per cellID
  data_sampled <- do.call("rbind", lapply(data_split, function(events) {
    if(nrow(events) > max_per_cell) {
      events[sample(1:nrow(events), max_per_cell), ]
    } else {
      events
    }
  }))

  # Split into groups after undersampling
  group_vectors <- list(
    STD_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "CON"],
    STD_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "STD" & data_sampled$adultTrt == "ALPS"],
    LBN_CON = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "CON"],
    LBN_ALPS = data_sampled$relPeak[data_sampled$earlyLifeTrt == "LBN" & data_sampled$adultTrt == "ALPS"]
  )
  
  return(group_vectors)
}

nIterations <- 50

STD_CON_vals = vector("list", nIterations)
STD_ALPS_vals = vector("list", nIterations)
LBN_CON_vals = vector("list", nIterations)
LBN_ALPS_vals = vector("list", nIterations)

# Perform the ad.test 500 times using undersampled data
for (i in 1:nIterations) {
  # Get the undersampled group vectors
  groups <- undersample_data_groups(pscProps)
  
  # Accumulate sample values across all bootstrap iterations
  STD_CON_vals[[i]] = groups$STD_CON
  STD_ALPS_vals[[i]] = groups$STD_ALPS
  LBN_CON_vals[[i]] = groups$LBN_CON
  LBN_ALPS_vals[[i]] = groups$LBN_ALPS
}

# doesn't ultimately matter, but sorted descending since this is relPeak
STD_CON_avg <- average_sorted_vectors(STD_CON_vals, sortDec = TRUE)
STD_ALPS_avg <- average_sorted_vectors(STD_ALPS_vals, sortDec = TRUE)
LBN_CON_avg <- average_sorted_vectors(LBN_CON_vals, sortDec = TRUE)
LBN_ALPS_avg <- average_sorted_vectors(LBN_ALPS_vals, sortDec = TRUE)


# Conduct the Anderson-Darling test
final_ad_test_result <- ad.test(
  x = list(
    STD_CON_avg
    , STD_ALPS_avg
    , LBN_CON_avg
    , LBN_ALPS_avg
  )
)

# Output the final test result
print(final_ad_test_result)
```

```{r}
# Convert average vectors to data frames
data_STD_CON <- data.frame(
  Value = STD_CON_avg
  , earlyLifeTrt = "STD"
  , adultTrt = "CON"
)
data_STD_ALPS <- data.frame(
  Value = STD_ALPS_avg
  , earlyLifeTrt = "STD"
  , adultTrt = "ALPS"
)
data_LBN_CON <- data.frame(
  Value = LBN_CON_avg
  , earlyLifeTrt = "LBN"
  , adultTrt = "CON"
)
data_LBN_ALPS <- data.frame(
  Value = LBN_ALPS_avg
  , earlyLifeTrt = "LBN"
  , adultTrt = "ALPS"
)

# Calculate ECDF for each group within the data
calc_ecdf <- function(data){
  data <- data %>%
    arrange(Value) %>%
    mutate(CumProb = ecdf(-Value)(-Value)
    #        ) %>%
    # mutate(Value = -Value # Flip the values for reverse plotting
           )  
  return(data)
}

# Apply the function to each data frame separately
data_STD_CON <- calc_ecdf(data_STD_CON)
data_STD_ALPS <- calc_ecdf(data_STD_ALPS)
data_LBN_CON <- calc_ecdf(data_LBN_CON)
data_LBN_ALPS <- calc_ecdf(data_LBN_ALPS)

# Combine all data frames into one
cdf_data <- rbind(data_STD_CON, data_STD_ALPS, data_LBN_CON, data_LBN_ALPS) %>%
  combineStress()

# Plot using ggplot2
plot_max15 <- ggplot(cdf_data, aes(x = Value, y = CumProb, color = comboTrt)) +
  geom_step() +
  scale_x_reverse() +
  labs(
    x = "relative peak (pA)", 
    y = "Cumulative Probability", 
    color = "Treatment"
  ) +
  boxTheme() + 
  textTheme(size = 16) +
  comboTrtLineColor() +
  coord_cartesian(xlim = c(0, -100))

plot_max100
plot_max15
```
```{r}
group_vectors <- list(
    STD_CON = pscProps$relPeak[pscProps$earlyLifeTrt == "STD" & pscProps$adultTrt == "CON"],
    STD_ALPS = pscProps$relPeak[pscProps$earlyLifeTrt == "STD" & pscProps$adultTrt == "ALPS"],
    LBN_CON = pscProps$relPeak[pscProps$earlyLifeTrt == "LBN" & pscProps$adultTrt == "CON"],
    LBN_ALPS = pscProps$relPeak[pscProps$earlyLifeTrt == "LBN" & pscProps$adultTrt == "ALPS"]
  )


# Convert average vectors to data frames
data_STD_CON <- data.frame(
  Value = group_vectors[["STD_CON"]]
  , earlyLifeTrt = "STD"
  , adultTrt = "CON"
)
data_STD_ALPS <- data.frame(
  Value = group_vectors[["STD_ALPS"]]
  , earlyLifeTrt = "STD"
  , adultTrt = "ALPS"
)
data_LBN_CON <- data.frame(
  Value = group_vectors[["LBN_CON"]]
  , earlyLifeTrt = "LBN"
  , adultTrt = "CON"
)
data_LBN_ALPS <- data.frame(
  Value = group_vectors[["LBN_ALPS"]]
  , earlyLifeTrt = "LBN"
  , adultTrt = "ALPS"
)

# Calculate ECDF for each group within the data
calc_ecdf <- function(data){
  data <- data %>%
    arrange(Value) %>%
    mutate(CumProb = ecdf(-Value)(-Value)
    #        ) %>%
    # mutate(Value = -Value # Flip the values for reverse plotting
           )  
  return(data)
}

# Apply the function to each data frame separately
data_STD_CON <- calc_ecdf(data_STD_CON)
data_STD_ALPS <- calc_ecdf(data_STD_ALPS)
data_LBN_CON <- calc_ecdf(data_LBN_CON)
data_LBN_ALPS <- calc_ecdf(data_LBN_ALPS)

# Combine all data frames into one
cdf_data <- rbind(data_STD_CON, data_STD_ALPS, data_LBN_CON, data_LBN_ALPS) %>%
  combineStress()

# Plot using ggplot2
ggplot(cdf_data, aes(x = Value, y = CumProb, color = comboTrt)) +
  geom_step() +
  scale_x_reverse() +
  labs(
    x = "relative peak (pA)", 
    y = "Cumulative Probability", 
    color = "Treatment"
  ) +
  boxTheme() + 
  textTheme(size = 16) +
  comboTrtLineColor() +
  coord_cartesian(xlim = c(0, -100))
```


```{r}


# Example usage:
vec1 <- c(30, 20, 50, 40)
vec2 <- c(25, 25, 15, 45)
vec3 <- c(35, 15, 25, 55) # Added a third vector for demonstration

# Combine the vectors into a list
list_of_vectors <- list(vec1, vec2, vec3)

# Call the average_sorted_vectors function
result_vector <- average_sorted_vectors(list_of_vectors)
result_vector
```

```{r}
emmeans(
  logriseTime_models
  , "adultTrt"
  , by = "earlyLifeTrt"
)

logriseTime_models$y


```

```{r}
lqmm(
  log10(riseTime) ~ earlyLifeTrt * adultTrt
  , random = ~ 1
  , group = cellID
  , tau = quantiles
  , data = pscProps
  , control = lqmmControl(
    LP_max_iter = 2000
  )
)
```
```{r}
logrelPeak_models_sum <- summary(logrelPeak_models)
```
# Summary 

```{r}
# ultimately need to set a seed before running these, I think, as it changes with each run
logrelPeak_models_sum <- summary(logrelPeak_models)
logdecay9010_models_sum <- summary(logdecay9010_models)
logFWHM_models_sum <- summary(logFWHM_models)
logriseTime_models_sum <- summary(logriseTime_models)
```

```{r}
logrelPeak_models_sum
logdecay9010_models_sum
logFWHM_models_sum
logriseTime_models_sum
```


```{r}
group_vectors <- list(
    STD_CON = pscProps$relPeak[pscProps$earlyLifeTrt == "STD" & pscProps$adultTrt == "CON"],
    STD_ALPS = pscProps$relPeak[pscProps$earlyLifeTrt == "STD" & pscProps$adultTrt == "ALPS"],
    LBN_CON = pscProps$relPeak[pscProps$earlyLifeTrt == "LBN" & pscProps$adultTrt == "CON"],
    LBN_ALPS = pscProps$relPeak[pscProps$earlyLifeTrt == "LBN" & pscProps$adultTrt == "ALPS"]
  )


# Convert average vectors to data frames
data_STD_CON <- data.frame(
  Value = group_vectors[["STD_CON"]]
  , earlyLifeTrt = "STD"
  , adultTrt = "CON"
)
data_STD_ALPS <- data.frame(
  Value = group_vectors[["STD_ALPS"]]
  , earlyLifeTrt = "STD"
  , adultTrt = "ALPS"
)
data_LBN_CON <- data.frame(
  Value = group_vectors[["LBN_CON"]]
  , earlyLifeTrt = "LBN"
  , adultTrt = "CON"
)
data_LBN_ALPS <- data.frame(
  Value = group_vectors[["LBN_ALPS"]]
  , earlyLifeTrt = "LBN"
  , adultTrt = "ALPS"
)

# Calculate ECDF for each group within the data
calc_ecdf <- function(data){
  data <- data %>%
    arrange(Value) %>%
    mutate(CumProb = ecdf(-Value)(-Value))  
  return(data)
}

# Apply the function to each data frame separately
data_STD_CON <- calc_ecdf(data_STD_CON)
data_STD_ALPS <- calc_ecdf(data_STD_ALPS)
data_LBN_CON <- calc_ecdf(data_LBN_CON)
data_LBN_ALPS <- calc_ecdf(data_LBN_ALPS)

# Combine all data frames into one
cdf_data <- rbind(data_STD_CON, data_STD_ALPS, data_LBN_CON, data_LBN_ALPS) %>%
  combineStress()

# Plot using ggplot2
ggplot(cdf_data, aes(x = Value, y = CumProb, color = comboTrt)) +
  geom_step() +
  scale_x_reverse() +
  labs(
    x = "relative peak (pA)", 
    y = "Cumulative Probability", 
    color = "Treatment"
  ) +
  boxTheme() + 
  textTheme(size = 16) +
  comboTrtLineColor() +
  coord_cartesian(xlim = c(0, -100))
```

```{r}
pscProps_log <- pscProps %>%
  mutate(
    logPeak = -log(-relPeak)
    , .after = relPeak
  ) %>%
  combineStress()

pscProps_log %>%
  arrange(- relPeak)
```

```{r}

pscProps_log %>%
  ggplot(aes(x = logPeak, color = comboTrt)) +
  stat_ecdf() +
  comboTrtLineColor() +
  coord_cartesian(
    xlim = c(-4.2, -3.8)
    , ylim = c(.2, .3)
  )
```

```{r}
# Create a new data frame for the prediction
new_data <- data.frame(
  earlyLifeTrt = factor(c("STD", "STD", "LBN", "LBN"), levels = c("STD", "LBN")),
  adultTrt = factor(c("CON", "ALPS", "CON", "ALPS"), levels = c("CON", "ALPS")),
  `(Intercept)` = 1
) %>%
  combineStress() %>%
  rename(
    `earlyLifeTrtLBN` = earlyLifeTrt
    ,`adultTrtALPS` = adultTrt
    ,`earlyLifeTrtLBN:adultTrtALPS` = comboTrt
  )

predict(logrelPeak_models, newdata = new_data)

pscProps %>%
  filter(
    earlyLifeTrt == "STD"
    , adultTrt == "ALPS"
  ) %>%
  group_by(
    cellID
  )

# Predict the conditional quantile for the combination of treatments
predicted_value <- predict(logrelPeak_models$)

# See the predicted value
predicted_value
```

```{r}
## Orthodont data
data(Orthodont)

# Random intercept model
fitOi.lqmm <- lqmm(distance ~ age, random = ~ 1, group = Subject,
	tau = c(0.1,0.5,0.9), data = Orthodont)

# Predict (y - Xb)	
predict(fitOi.lqmm, level = 0)

# Predict (y - Xb - Zu)
predict(fitOi.lqmm, level = 1)

# 95% confidence intervals
predint(fitOi.lqmm, level = 0, alpha = 0.05)

```

```{r}
pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = comboTrt,
      y = relPeak,
      # color = cellID
      # , fill = cellID
      color = -log(-relPeak)
      , fill = -log(-relPeak)
    )
  ) +
  jitterGeom(
    size = .5,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  expand_limits(y=0)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.title.x = element_blank()
  ) + 
  labs(y = "-log(-amplitude (pA))") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis()
```
```{r}
pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = comboTrt,
      y = -log(-relPeak),
      # color = cellID
      # , fill = cellID
      color = -log(-relPeak)
      , fill = -log(-relPeak)
    )
  ) +
  jitterGeom(
    size = .5,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  expand_limits(y=0)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.title.x = element_blank()
  ) + 
  labs(y = "-log(-amplitude (pA))") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis()
```
```{r}
pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = cellID,
      y = -log(-relPeak),
      color = -log(-relPeak)
      , fill = -log(-relPeak)
    )
  ) +
  jitterGeom(
    size = 1,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  expand_limits(y=0)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.title.x = element_blank()
    , axis.text.x = element_blank()
  ) + 
  labs(y = "-log(-amplitude (pA))") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  )
```
```{r}
# Define the custom transformation
neg_log_trans <- trans_new(
  name = "neg_log",
  transform = function(y) -log10(-y),
  inverse = function(y) -10^(-y)
)

pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = cellID,
      y = relPeak,
      color = -log(-relPeak)
      , fill = -log(-relPeak)
    )
  ) +
  jitterGeom(
    size = 1,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  expand_limits(y=-1)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.title.x = element_blank()
    , axis.text.x = element_blank()
  ) + 
  labs(y = "-log(-amplitude (pA))") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  ) +
  scale_y_continuous(trans = neg_log_trans, labels = scales::math_format())
```
```{r}
pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = cellID,
      y = relPeak,
      color = -log(-relPeak)
      , fill = -log(-relPeak)
    )
  ) +
  jitterGeom(
    size = 1,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  expand_limits(y=-1)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.title.x = element_blank()
    , axis.text.x = element_blank()
  ) + 
  labs(y = "-log(-amplitude (pA))") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  ) +
  scale_y_continuous(trans = neg_log_trans, breaks = trans_breaks("neg_log", function(x) -10^(-x)), labels = trans_format("neg_log", math_format(-10^.x)))
```
```{r}
# Define the custom transformation
neg_log_trans <- trans_new(
  name = "neg_log"
  , transform = function(y) -log10(-y)
  , inverse = function(y) -10^(-y)
  # , breaks = c(-1, -10, -50, -100, -150, -200, -250, -300, -350, -400)
  , domain = c(-Inf, -0.0001)
)

df_counts <- pscProps %>%
  group_by(cellID) %>%
    summarize(
      count = n()
      , .groups = "drop"
    ) %>%
    arrange(
      desc(count)
    )

pscProps %>%
  mutate(
    cellID = factor(
      cellID
      , levels = df_counts$cellID
    )
  ) %>%
  combineStress() %>%
  ggplot(
    aes(
      x = cellID,
      y = relPeak,
      color = -log(-relPeak)
      , fill = -log(-relPeak)
    )
  ) +
  jitterGeom(
    size = .75,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  # expand_limits(y=-1)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    # axis.title.x = element_blank()
    axis.text.x = element_blank()
  ) + 
  labs(y = "amplitude (pA)", x = "cell") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  ) +
  scale_y_continuous(
    trans = neg_log_trans
    , breaks = c(-2.5, -5, -10, -20, -40, -80, -160, -320, -480)
  )
```

```{r}
pscProps %>%
  combineStress() %>%
  ggplot(
    aes(
      x = comboTrt,
      y = relPeak,
      color = cellID
      , fill = cellID
    )
  ) +
  jitterGeom(
    size = .75,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  # expand_limits(y=0)+
  # coord_cartesian(if(zoom_x){xlim = c(xmin, xmax)}, if(zoom_y){ylim = c(ymin, ymax)})+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.title.x = element_blank()
  ) + 
  labs(y = "amplitude (pA)") +
  scale_color_viridis(discrete = TRUE) +
  scale_fill_viridis(discrete = TRUE) +
  scale_y_continuous(
    trans = neg_log_trans
    , breaks = c(-2.5, -5, -10, -20, -40, -80, -160, -320, -480)
  )
```

```{r}
pscProps %>%
  mutate(
    cellID = factor(
      cellID
      , levels = df_counts$cellID
    )
  ) %>%
  combineStress() %>%
  ggplot(
    aes(
      x = cellID,
      y = riseTime,
      color = log(riseTime)
      , fill = log(riseTime)
    )
  ) +
  jitterGeom(
    size = .75,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    # axis.title.x = element_blank()
    axis.text.x = element_blank()
  ) + 
  labs(y = "rise time (ms)", x = "cell") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  # ) +
  # scale_y_log10(
  #   breaks = c(0.00125, 0.0025, 0.005, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12)
  #   , labels = c("0.00125", "0.0025", "0.005", "0.01", "0.02", "0.04", "0.08", "0.16", "0.32", "0.64", "1.28", "2.56", "5.12")
  )
```
```{r}
pscProps %>%
  mutate(
    cellID = factor(
      cellID
      , levels = df_counts$cellID
    )
  ) %>%
  combineStress() %>%
  ggplot(
    aes(
      x = cellID,
      y = riseTime,
      color = log(riseTime)
      , fill = log(riseTime)
    )
  ) +
  jitterGeom(
    size = .75,
    alpha = .6,
    width = .35,
    height = 0
  ) +
  theme_pubr()+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    # axis.title.x = element_blank()
    axis.text.x = element_blank()
  ) + 
  labs(y = "rise time (ms)", x = "cell") +
  # scale_color_viridis(discrete = TRUE) +
  # scale_fill_viridis(discrete = TRUE)
  scale_color_viridis() +
  scale_fill_viridis() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  ) +
  scale_y_log10(
    breaks = c(0.00125, 0.0025, 0.005, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12)
    , labels = c("0.00125", "0.0025", "0.005", "0.01", "0.02", "0.04", "0.08", "0.16", "0.32", "0.64", "1.28", "2.56", "5.12")
  )
```

```{r}
pscProps %>%
  filter(
    riseTime < 0.01
  )
pscProps %>%
  filter(
    riseTime < 0.1
  )
```

# Add Quartile Colors Function
```{r}
add_quartile_colors <- function(
    data
    , group_col
    , value_col
    , reverse_order = FALSE
) {
  data <- data %>%
    group_by({{ group_col }}) %>%
    mutate(
      QuartileID = cut({{ value_col }},
                       breaks = quantile({{ value_col }}, probs = 0:4/4, na.rm = TRUE),
                       include.lowest = TRUE,
                       labels = FALSE)
    ) %>% 
    ungroup()
  
  # quartile_colors <- viridis(4, option = "D")
  quartile_colors <- c("1" = "#2c7bb6", "2" = "#abd9e9", "3" = "#fdae61", "4" = "#d7191c")
  if (reverse_order) {
    color_assignment <- (4 - as.numeric(data$QuartileID)) +1
  } else {
    color_assignment <- as.numeric(data$QuartileID)
  }

  data$Color <- quartile_colors[color_assignment]

  data <- data %>%
    select(-QuartileID)

    # mutate(
    #   Color = viridis(4, option = "D")[as.numeric(QuartileID)]
    # ) %>%
    # select(-QuartileID)

  return(data)
}

pscProps %>%
  add_quartile_colors(
    cellID
    , relPeak
    , reverse_order = TRUE
  )
```



# Plotting function - log

```{r}
plotPSCProp_log <- function(
  df
  , yVar
  , yLab
  , logBreaks
  , logLabels
  , byCell = TRUE
  , dotSize = 0.75
  , dotAlpha = 0.6
  , jitterWidth = 0.35
  , byQuartiles = TRUE
  , sortByQuartile = TRUE
){
  if(byCell){
    if(sortByQuartile){
      df_counts <- df %>%
        group_by(cellID) %>%
          summarize(
            quartile25 = quantile({{ yVar }}, probs = 0.25, na.rm = TRUE)
            , .groups = "drop"
          ) %>%
          arrange(
            desc(quartile25)
          )
      
    } else {
      df_counts <- df %>%
        group_by(cellID) %>%
          summarize(
            count = n()
            , .groups = "drop"
          ) %>%
          arrange(
            desc(count)
          )
      
    }
    
    df <- df %>%
      mutate(
        cellID = factor(
          cellID
          , levels = df_counts$cellID
        )
      )
  }
  
  df <- df %>%
    combineStress()
  
  if(byQuartiles){
    df <- df %>%
      add_quartile_colors(
        if(byCell){cellID} else {comboTrt}
        , value_col = {{ yVar }}
      )
  }
  
  viz <- df %>%
    ggplot(
      aes(
        x = if(byCell){cellID} else{comboTrt}
        , y = {{ yVar }}
        , color = if(byQuartiles){Color} else {log({{ yVar }})}
        , fill = if(byQuartiles){Color} else {log({{ yVar }})}
      )
  ) +
  jitterGeom(
    size = dotSize,
    alpha = dotAlpha,
    width = jitterWidth,
    height = 0
  ) +
  theme_pubr()+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.text.x = element_blank()
  ) + 
  labs(y = yLab, x = ifelse(byCell, "cell", ""))
  
  if(byQuartiles){
    viz <- viz +
      scale_color_identity() +
      scale_fill_identity()
  } else {
    viz <- viz +
      scale_color_viridis() +
      scale_fill_viridis()
  }
  
  viz <- viz +
    facet_wrap(
      ~ comboTrt
      , nrow = 1
      , scales = "free_x"
      , strip.position = "bottom"
    ) +
    scale_y_log10(
      breaks = logBreaks
      , labels = logLabels
    )
  
  return(viz)
}
```

# Plotting function - neg log - quartiles

```{r}
plotPSCProp_negLog_quartiles <- function(
  df
  , yVar
  , yLab
  , logBreaks = c(-2.5, -5, -10, -20, -40, -80, -160, -320, -480)
  , logLabels = c("-2.5", "-5", "-10", "-20", "-40", "-80", "-160", "-320", "-480")
  , byCell = TRUE
  , dotSize = 0.75
  , dotAlpha = 0.6
  , jitterWidth = 0.35
){
  # Define the custom transformation
  neg_log_trans <- trans_new(
    name = "neg_log"
    , transform = function(y) -log10(-y)
    , inverse = function(y) -10^(-y)
    , domain = c(-Inf, -0.0001)
  )
  
  if(byCell){
    df_counts <- df %>%
      group_by(cellID) %>%
        summarize(
          count = n()
          , .groups = "drop"
        ) %>%
        arrange(
          desc(count)
        )
    
    df <- df %>%
      mutate(
        cellID = factor(
          cellID
          , levels = df_counts$cellID
        )
      )
  }
  
  df <- df %>%
    combineStress() %>%
    add_quartile_colors(
      if(byCell){cellID} else {comboTrt}
      , value_col = {{ yVar }}
    )
  
  viz <- df %>%
    ggplot(
      aes(
        x = if(byCell){cellID} else{comboTrt}
        , y = {{ yVar }}
        , color = Color
        , fill = Color
      )
  ) +
  jitterGeom(
    size = dotSize,
    alpha = dotAlpha,
    width = jitterWidth,
    height = 0
  ) +
  theme_pubr()+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.text.x = element_blank()
  ) + 
  labs(y = yLab, x = ifelse(byCell, "cell", "")) +
  scale_color_identity() +
  scale_fill_identity() +
  facet_wrap(
    ~ comboTrt
    , nrow = 1
    , scales = "free_x"
  ) +
  scale_y_continuous(
    trans = neg_log_trans
    , breaks = logBreaks
    , labels = logLabels
  )
  
  return(viz)
}
```

# Plotting function - neg log

```{r}
plotPSCProp_negLog <- function(
  df
  , yVar
  , yLab
  , logBreaks = c(-2.5, -5, -10, -20, -40, -80, -160, -320, -480)
  , logLabels = c("-2.5", "-5", "-10", "-20", "-40", "-80", "-160", "-320", "-480")
  , byCell = TRUE
  , dotSize = 0.75
  , dotAlpha = 0.6
  , jitterWidth = 0.35
  , byQuartiles = TRUE
  , reverseColor = FALSE
  , sortByQuartile = TRUE
){
  # Define the custom transformation
  neg_log_trans <- trans_new(
    name = "neg_log"
    , transform = function(y) -log10(-y)
    , inverse = function(y) -10^(-y)
    , domain = c(-Inf, -0.0001)
  )
  
  if(byCell){
    if(sortByQuartile){
      df_counts <- df %>%
        group_by(cellID) %>%
          summarize(
            quartile25 = quantile({{ yVar }}, probs = 0.25, na.rm = TRUE)
            , .groups = "drop"
          ) %>%
          arrange(
            desc(quartile25)
          )
      
    } else {
      df_counts <- df %>%
        group_by(cellID) %>%
          summarize(
            count = n()
            , .groups = "drop"
          ) %>%
          arrange(
            desc(count)
          )
      
    }
    
    df <- df %>%
      mutate(
        cellID = factor(
          cellID
          , levels = df_counts$cellID
        )
      )
  }
  
  df <- df %>%
    combineStress()
  
  if(byQuartiles){
    df <- df %>%
      add_quartile_colors(
        if(byCell){cellID} else {comboTrt}
        , value_col = {{ yVar }}
        , reverse_order = reverseColor
      )
  }
  
  viz <- df %>%
    ggplot(
      aes(
        x = if(byCell){cellID} else{comboTrt}
        , y = {{ yVar }}
        , color = if(byQuartiles){Color} else {- log(- {{ yVar }})}
        , fill = if(byQuartiles){Color} else {- log(- {{ yVar }})}
      )
  ) +
  jitterGeom(
    size = dotSize,
    alpha = dotAlpha,
    width = jitterWidth,
    height = 0
  ) +
  theme_pubr()+
  theme(
    legend.position = "none"
  )+
  textTheme(size = textSize)+
  boxTheme() +
  theme(
    axis.text.x = element_blank()
  ) + 
  labs(y = yLab, x = ifelse(byCell, "cell", ""))
  
  if(byQuartiles){
    viz <- viz +
      scale_color_identity() +
      scale_fill_identity()
  } else {
    viz <- viz +
      scale_color_viridis() +
      scale_fill_viridis()
  }
  
  viz <- viz +
    facet_wrap(
      ~ comboTrt
      , nrow = 1
      , scales = "free_x"
      , strip.position = "bottom"
    ) +
    scale_y_continuous(
      trans = neg_log_trans
      , breaks = logBreaks
      , labels = logLabels
    )
  
  return(viz)
}
```


```{r}
pscProps <- pscProps %>%
  mutate(
    absAmp = -relPeak
  )
```

# AMPLITUDE

## By cell

Be careful with reversing color, if the percentiles for the model are not reversed

```{r}
relPeak_byCell <- pscProps %>%
  # plotPSCProp_negLog(
  #   yVar = relPeak
  #   , yLab = "amplitude (pA)"
  #   , byCell = TRUE
  #   , reverseColor = FALSE
  # )
  plotPSCProp_log(
    yVar = absAmp
    , yLab = "amplitude (pA)"
    , logBreaks = c(5, 10, 20, 40, 80, 160, 320, 480)
    , logLabels = c("5", "10", "20", "40", "80", "160", "320", "480")
    , byCell = TRUE
  )

relPeak_byCell
```

## By treatment

```{r}
relPeak_byTrt <- pscProps %>%
  # plotPSCProp_negLog(
  #   yVar = relPeak
  #   , yLab = "amplitude (pA)"
  #   , byCell = FALSE
  #   , reverseColor = FALSE
  # )
  plotPSCProp_log(
    yVar = absAmp
    , yLab = "amplitude (pA)"
    , logBreaks = c(5, 10, 20, 40, 80, 160, 320, 480)
    , logLabels = c("5", "10", "20", "40", "80", "160", "320", "480")
    , byCell = FALSE
  )

relPeak_byTrt
```

# RISE TIME

## By Cell

```{r}
riseTime_byCell <- pscProps %>%
  plotPSCProp_log(
    yVar = riseTime
    , yLab = "rise time (ms)"
    , logBreaks = c(0.00125, 0.0025, 0.005, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12)
    , logLabels = c("0.00125", "0.0025", "0.005", "0.01", "0.02", "0.04", "0.08", "0.16", "0.32", "0.64", "1.28", "2.56", "5.12")
    , byCell = TRUE
  )

riseTime_byCell
```

## By treatment
```{r}
riseTime_byTrt <- pscProps %>%
  plotPSCProp_log(
    yVar = riseTime
    , yLab = "rise time (ms)"
    , logBreaks = c(0.00125, 0.0025, 0.005, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12)
    , logLabels = c("0.00125", "0.0025", "0.005", "0.01", "0.02", "0.04", "0.08", "0.16", "0.32", "0.64", "1.28", "2.56", "5.12")
    , byCell = FALSE
  )
riseTime_byTrt
```

# DECAY TIME

## By Cell

```{r}
decayTime_byCell <- pscProps %>%
  plotPSCProp_log(
    yVar = decay9010
    , yLab = "decay time (ms)"
    , logBreaks = c(0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24, 20.48, 40.96, 81.92, 163.84)
    , logLabels = c("0.08", "0.16", "0.32", "0.64", "1.28", "2.56", "5.12", "10.24", "20.48", "40.96", "81.92", "163.84")
    , byCell = TRUE
  )
decayTime_byCell
```

## By treatment
```{r}
decayTime_byTrt <- pscProps %>%
  plotPSCProp_log(
    yVar = decay9010
    , yLab = "decay time (ms)"
    , logBreaks = c(0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24, 20.48, 40.96, 81.92, 163.84)
    , logLabels = c("0.08", "0.16", "0.32", "0.64", "1.28", "2.56", "5.12", "10.24", "20.48", "40.96", "81.92", "163.84")
    , byCell = FALSE
  )
decayTime_byTrt
```
# FWHM

## By Cell

```{r}
fwhm_byCell <- pscProps %>%
  plotPSCProp_log(
    yVar = fwhm
    , yLab = "fwhm (ms)"
    , logBreaks = c(0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24, 20.48, 40.96)
    , logLabels = c("0.16", "0.32", "0.64", "1.28", "2.56", "5.12", "10.24", "20.48", "40.96")
    , byCell = TRUE
  ) +
  expand_limits(y = 0.16)

fwhm_byCell
```

## By treatment
```{r}
fwhm_byTrt <- pscProps %>%
  plotPSCProp_log(
    yVar = fwhm
    , yLab = "fwhm (ms)"
    , logBreaks = c(0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24, 20.48, 40.96)
    , logLabels = c("0.16", "0.32", "0.64", "1.28", "2.56", "5.12", "10.24", "20.48", "40.96")
    , byCell = FALSE
  ) +
  expand_limits(y = 0.16)

fwhm_byTrt
```

```{r}
pscProps %>%
  filter(
    relPeak > -5
  )
```


# Set-up
```{r}
figureNum <- 1

pptBaseName <- makeBaseNameFunc("")

plotFolder <- file.path(plotOutputFolder, "GABA_PSC_props")

imgType <-"png"

exportImg <- exportImg_forPurposeFunc(
  imgType = imgType
  , figNumFunc = pptBaseName
  , plotFolder = plotFolder
  , compType = currentCompType
)

exportFullPPTSlide <- function(
    plot = last_plot()
    , baseName = fileBaseName
    , figNum = figureNum
    , width = 11.5
    , height = 5
){
  exportImg(
    plot = plot
    , fileBaseName = baseName
    , figNum = figNum
    , units = "in"
    , width = width
    , height = height
  )
}

exportHalfPPTSlide <- function(
    plot = last_plot()
    , baseName = fileBaseName
    , figNum = figureNum
){
  exportImg(
    plot = plot
    , fileBaseName = baseName
    , figNum = figNum
    , units = "in"
    , width = 5.67
    , height = 5
  )
}

exportThirdPPTSlide <- function(
    plot = last_plot()
    , baseName = fileBaseName
    , figNum = figureNum
){
  exportImg(
    plot = plot
    , fileBaseName = baseName
    , figNum = figNum
    , units = "in"
    , width = 3.6
    , height = 5
  )
}

exportQuarterPPTSlide <- function(
    plot = last_plot()
    , baseName = fileBaseName
    , figNum = figureNum
){
  exportImg(
    plot = plot
    , fileBaseName = baseName
    , figNum = figNum
    , units = "in"
    , width = 3.2
    , height = 5
  )
}

editableImgs <- TRUE
pptAddOneGraph <- function(
    title = slideTitle
    , plot = last_plot()
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_oneGraph(
    officer_ppt, 
    title, 
    plot, 
    makeEditable = editableImgs
  )
  return(officer_ppt)
}

pptAddOneTable <- function(
    table
    , title = slideTitle
    , officer_ppt = ppt
    , dontFormat = TRUE
){
  officer_ppt <- addSlide_oneTable(
    officer_ppt, 
    title, 
    table, 
    dontFormat = dontFormat
  )
  return(officer_ppt)
}

pptUneditAddOneGraph <- function(
    title = slideTitle
    , plot = last_plot()
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_oneGraph(
    officer_ppt, 
    title, 
    plot, 
    makeEditable = FALSE
  )
  return(officer_ppt)
}

pptAddTwoGraphs <- function(
    plot1
    , plot2
    , title = slideTitle
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_twoGraph(
    officer_ppt, 
    title, 
    plot1, 
    plot2,
    makeEditable = editableImgs
  )
  return(officer_ppt)
}

pptAddTwoGraphsMoreLeft <- function(
    plot1
    , plot2
    , title = slideTitle
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_twoGraphMoreLeft(
    officer_ppt, 
    title, 
    plot1, 
    plot2,
    makeEditable = editableImgs
  )
  return(officer_ppt)
}

pptAddGraphTable <- function(
    plot
    , table
    , title = slideTitle
    , officer_ppt = ppt
    , dontFormat = TRUE
){
  officer_ppt <- addSlide_graphTable(
    officer_ppt, 
    title, 
    plot, 
    table,
    makeEditable = editableImgs
    , dontFormat = dontFormat
    , textSize = textSize
  )
  return(officer_ppt)
}

pptAddFourGraphs <- function(
    plot1
    , plot2
    , plot3
    , plot4
    , title = slideTitle
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_fourGraph(
    officer_ppt, 
    title, 
    plot1, 
    plot2, 
    plot3,
    plot4,
    makeEditable = editableImgs
    )
  return(officer_ppt)
}

pptAddThreeGraphs <- function(
    plot1
    , plot2
    , plot3
    , title = slideTitle
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_threeGraph(
    officer_ppt, 
    title, 
    plot1, 
    plot2, 
    plot3,
    makeEditable = editableImgs
    )
  return(officer_ppt)
}

pptAddStats <- function(
    statsText
    , officer_ppt = ppt
){
  officer_ppt <- addStatsToBottom(
    officer_ppt,
    statsText
  )
}

pptAddTwoStats <- function(
    statsText1
    , statsText2
    , officer_ppt = ppt
){
  officer_ppt <- addTwoStatsToGraphs(
    officer_ppt,
    statsText1,
    statsText2
  )
}

pptAddSectionHead <- function(
    title = slideTitle
    , officer_ppt = ppt
){
  officer_ppt <- addSlide_sectionHead(
    officer_ppt
    , title
  )
}



```

```{r}
# make powerpoint and title slide
ppt <- read_pptx("genericPPT.pptx")
ppt <- add_slide(ppt, layout = "Title Slide")

# layout_properties(ppt, layout="Title Slide")

subText <- "PSC properties"
ppt <- ph_with(
  ppt,
  value = subText,
  location = ph_location_label("Subtitle 2")
)


# Run this function to save the PPT file to the disk
# On windows, don't run if the PPT file is open
savePPT <- makeSavePPTFunc(
  presPPT = ppt
  , presFolder = plotFolder
  , presFileName = "AGG_LBN_PSC_props"
  , addDate = TRUE
)
```

```{r}
relPeak_byCell
fileBaseName <- "relPeak_byCell"
slideTitle <- "Amplitude"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
relPeak_byTrt
fileBaseName <- "relPeak_byTrt"
slideTitle <- "Amplitude"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
riseTime_byCell
fileBaseName <- "riseTime_byCell"
slideTitle <- "Rise time"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
riseTime_byTrt
fileBaseName <- "riseTime_byTrt"
slideTitle <- "Rise time"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
decayTime_byCell
fileBaseName <- "decayTime_byCell"
slideTitle <- "Decay time"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
decayTime_byTrt
fileBaseName <- "decayTime_byTrt"
slideTitle <- "Decay time"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
fwhm_byCell
fileBaseName <- "fwhm_byCell"
slideTitle <- "FWHM"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```
```{r}
fwhm_byTrt
fileBaseName <- "fwhm_byTrt"
slideTitle <- "FWHM"
exportFullPPTSlide()
ppt <- pptAddOneGraph()
```

```{r}
logAmplitude_models_sum$tTable[[1]]

rownames(logAmplitude_models_sum$tTable[[1]])

logAmplitude_models_sum$tTable[[1]]
  
simplifyQuartileOutput <- function(df # just one quartile
                                   ){
  df %>%
    subEarlyLifeTrtInRowNames_quartile() %>%
    subAdultTrtInRowNames_quartile() %>%
    subColonInRowNames() %>%
    as.data.frame() %>%
    rownames_to_column(var = "fixed effect") %>%
    rename(
      SEM = `Std. Error`
      , p = `Pr(>|t|)`
    ) %>%
    mutate(
      Value = 10^Value
      , SEM = 10^SEM
      , `95% CI` = paste0(
        "[", format(
          round(
            10^`lower bound`
            , 2
          )
          , nsmall = 2, trim = TRUE
        ), ", ", format(
          round(
            10^`upper bound`
            , 2
          )
          , nsmall = 2, trim = TRUE
        ), "]")
      , .after = SEM
    ) %>% 
    select(
      -c(`lower bound`, `upper bound`)
    ) %>%
    formatPCol()
}

simplifyAllQuartilesOutput <- function(allDFs){
  
  resultDF <- NULL
  
  for(name in names(allDFs)){
    df <- allDFs[[name]]
    
    simpDF <- simplifyQuartileOutput(df) %>%
      mutate(
        quartile = name
        , .before = `fixed effect`
      )
    
     resultDF <- bind_rows(resultDF, simpDF)
  }
  
  return(resultDF)
}

logAmplitude_models_sum$tTable %>%
  simplifyAllQuartilesOutput()


names(logAmplitude_models_sum$tTable)

for(name in names(logAmplitude_models_sum$tTable)){
  print(name)
}

logAmplitude_models_sum$tTable[["0.25"]]

logAmplitude_models_sum$tTable[[]] %>%
  simplifyAllQuartilesOutput()
  simplifyQuartileOutput()

pscProps$earlyLifeTrt <- factor(pscProps$earlyLifeTrt, levels = c("STD", "LBN"))

bind_rows(
  list(
    "0.25" = logAmplitude_models_sum$tTable[[1]] %>% as.tibble()
    , "0.5" = logAmplitude_models_sum$tTable[[2]] %>% as.tibble()
    , "0.75" = logAmplitude_models_sum$tTable[[3]] %>% as.tibble()
  )
  , .id = "percentile"
)
```



```{r}
names(logModels_tbl)
```
Something odd about the Q3 estimate for FWHM, way too large. Not sure why, but there are also some differences in the Covariance matrix of the random effects
```{r}
pscProps_log <- pscProps %>%
  mutate(
    logFWHM = log10(fwhm)
    , logRiseTime = log10(riseTime)
    , logDecayTime = log10(decay9010)
    , logAmplitude = log10(amplitude)
  )

pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    logFWHM
  )
pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    logRiseTime
  )
pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    logDecayTime
  )
pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    logAmplitude
  )

pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    fwhm
  )
pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    riseTime
  )
pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    decay9010
  )
pscProps_log %>%
  group_by(
    earlyLifeTrt
    , adultTrt
  ) %>%
  quartilesSummary(
    amplitude
  )

lqmm(
  logFWHM ~ earlyLifeTrt * adultTrt
  , random = ~1
  , group = cellID
  , tau = quantiles
  , data = pscProps_log
  , control = lqmmControl(
    LP_max_iter = 2500
  )
)
pscProps_log
```

```{r}
predict(logFWHM_models) %>%
  as.data.frame()
```

